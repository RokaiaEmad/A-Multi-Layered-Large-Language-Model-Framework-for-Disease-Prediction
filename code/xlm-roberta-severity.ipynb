{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T15:15:38.635277Z",
     "iopub.status.busy": "2025-06-16T15:15:38.635020Z",
     "iopub.status.idle": "2025-06-16T15:15:38.642276Z",
     "shell.execute_reply": "2025-06-16T15:15:38.641667Z",
     "shell.execute_reply.started": "2025-06-16T15:15:38.635256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"c26df6b59bfb128917e73bbb00a79ca7e9324a11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T15:31:21.867962Z",
     "iopub.status.busy": "2025-06-16T15:31:21.867678Z",
     "iopub.status.idle": "2025-06-16T21:26:27.162048Z",
     "shell.execute_reply": "2025-06-16T21:26:27.161261Z",
     "shell.execute_reply.started": "2025-06-16T15:31:21.867942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Training Severity Distribution:\n",
      "severity\n",
      "ÿ≠ÿ±ÿ¨        120301\n",
      "ÿ∫Ÿäÿ± ÿ≠ÿ±ÿ¨     38842\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîπ Test Severity Distribution:\n",
      "severity\n",
      "ÿ≠ÿ±ÿ¨        30075\n",
      "ÿ∫Ÿäÿ± ÿ≠ÿ±ÿ¨     9711\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98797cb5587d4d63b75e0dd53d1baf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ab53db1d604738b3b84d61e7e068a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39786 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/4261496388.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8290' max='8290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8290/8290 5:46:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.345586</td>\n",
       "      <td>0.840924</td>\n",
       "      <td>0.843063</td>\n",
       "      <td>0.840924</td>\n",
       "      <td>0.841911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.378035</td>\n",
       "      <td>0.826622</td>\n",
       "      <td>0.855160</td>\n",
       "      <td>0.826622</td>\n",
       "      <td>0.834433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.374705</td>\n",
       "      <td>0.835998</td>\n",
       "      <td>0.862674</td>\n",
       "      <td>0.835998</td>\n",
       "      <td>0.843201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.356396</td>\n",
       "      <td>0.853944</td>\n",
       "      <td>0.867076</td>\n",
       "      <td>0.853944</td>\n",
       "      <td>0.858201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.571796</td>\n",
       "      <td>0.803197</td>\n",
       "      <td>0.863407</td>\n",
       "      <td>0.803197</td>\n",
       "      <td>0.815488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.156700</td>\n",
       "      <td>0.467305</td>\n",
       "      <td>0.834389</td>\n",
       "      <td>0.867244</td>\n",
       "      <td>0.834389</td>\n",
       "      <td>0.842510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.553272</td>\n",
       "      <td>0.829387</td>\n",
       "      <td>0.866731</td>\n",
       "      <td>0.829387</td>\n",
       "      <td>0.838259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.639973</td>\n",
       "      <td>0.825190</td>\n",
       "      <td>0.866794</td>\n",
       "      <td>0.825190</td>\n",
       "      <td>0.834718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.842759</td>\n",
       "      <td>0.870679</td>\n",
       "      <td>0.842759</td>\n",
       "      <td>0.849906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.684867</td>\n",
       "      <td>0.838134</td>\n",
       "      <td>0.869379</td>\n",
       "      <td>0.838134</td>\n",
       "      <td>0.845901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Test Metrics: {'eval_loss': 0.35639625787734985, 'eval_accuracy': 0.8539435982506409, 'eval_precision': 0.8670760117303695, 'eval_recall': 0.8539435982506409, 'eval_f1': 0.8582014389961218, 'eval_runtime': 166.7611, 'eval_samples_per_second': 238.581, 'eval_steps_per_second': 1.247, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Confusion Matrix:\n",
      "[[26183  3892]\n",
      " [ 1919  7792]]\n",
      "\n",
      "üîπ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90     30075\n",
      "           1       0.67      0.80      0.73      9711\n",
      "\n",
      "    accuracy                           0.85     39786\n",
      "   macro avg       0.80      0.84      0.81     39786\n",
      "weighted avg       0.87      0.85      0.86     39786\n",
      "\n",
      "‚úÖ Model Loaded Successfully!\n",
      "\n",
      "üîπ Input: ÿßŸÑÿßÿπÿ±ÿßÿ∂ ÿ¥ÿØŸäÿØŸá\n",
      "Predicted Severity: ÿ≠ÿ±ÿ¨\n",
      "\n",
      "üîπ Input: ÿ™ÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸâ ÿ±ÿßÿ≠Ÿá ŸÅŸÇÿ∑\n",
      "Predicted Severity: ÿ≠ÿ±ÿ¨\n",
      "\n",
      "üîπ Input: ÿ≠ÿßŸÑŸá ŸÖÿ™Ÿàÿ≥ÿ∑Ÿá\n",
      "Predicted Severity: ÿ≠ÿ±ÿ¨\n",
      "\n",
      "üîπ Input: ŸÜÿ≤ŸäŸÅ ÿ≠ÿßÿØ\n",
      "Predicted Severity: ÿ≠ÿ±ÿ¨\n",
      "\n",
      "‚úÖ Model Saved & Zipped for Download!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='XLM-RoBERTa-Severity.zip' target='_blank'>XLM-RoBERTa-Severity.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/XLM-RoBERTa-Severity.zip"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "train_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Train.xlsx\"\n",
    "test_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Test.xlsx\"\n",
    "\n",
    "train_df = pd.read_excel(train_path)\n",
    "test_df = pd.read_excel(test_path)\n",
    "\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)[['q_body', 'severity', 'category']]\n",
    "\n",
    "valid_categories = [\n",
    "    \"ÿßŸÖÿ±ÿßÿ∂ ŸÜÿ≥ÿßÿ¶Ÿäÿ©\", \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπÿ∂ŸÑÿßÿ™ ŸàÿßŸÑÿπÿ∏ÿßŸÖ Ÿà ÿßŸÑŸÖŸÅÿßÿµŸÑ\", \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨Ÿáÿßÿ≤ ÿßŸÑŸáÿ∂ŸÖŸä\",\n",
    "    \"ÿßŸÑÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©\", \"ÿ∑ÿ® ÿßŸÑÿßÿ≥ŸÜÿßŸÜ\", \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑŸÇŸÑÿ® Ÿà ÿßŸÑÿ¥ÿ±ÿßŸäŸäŸÜ\",\n",
    "    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπŸäŸàŸÜ\", \"ÿßŸÜŸÅ ÿßÿ∞ŸÜ Ÿàÿ≠ŸÜÿ¨ÿ±ÿ©\", \"ÿ¨ÿ±ÿßÿ≠ÿ© ÿ™ÿ¨ŸÖŸäŸÑ\", \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿØŸÖ\"\n",
    "]\n",
    "all_data = all_data[all_data[\"category\"].isin(valid_categories)].reset_index(drop=True)\n",
    "\n",
    "valid_severity = [\"ÿ≠ÿ±ÿ¨\", \"ÿ∫Ÿäÿ± ÿ≠ÿ±ÿ¨\"]\n",
    "all_data = all_data[all_data[\"severity\"].isin(valid_severity)].reset_index(drop=True)\n",
    "\n",
    "all_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_df, test_df = train_test_split(\n",
    "    all_data[['q_body', 'severity']],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=all_data['severity']\n",
    ")\n",
    "\n",
    "print(\"\\nüîπ Training Severity Distribution:\")\n",
    "print(train_df[\"severity\"].value_counts())\n",
    "print(\"\\nüîπ Test Severity Distribution:\")\n",
    "print(test_df[\"severity\"].value_counts())\n",
    "\n",
    "severity_mapping = {sev: i for i, sev in enumerate(valid_severity)}\n",
    "train_df['label'] = train_df['severity'].map(severity_mapping)\n",
    "test_df['label'] = test_df['severity'].map(severity_mapping)\n",
    "\n",
    "model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"q_body\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['q_body', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['q_body', 'label']])\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = self.loss_fn(logits, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = CustomModel(base_model, num_labels=len(valid_severity))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=96,\n",
    "    per_device_eval_batch_size=96,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=3e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nüîπ Starting Training...\")\n",
    "trainer.train()\n",
    "\n",
    "test_metrics = trainer.evaluate(test_dataset)\n",
    "print(\"\\nüîπ Test Metrics:\", test_metrics)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "print(\"\\nüîπ Confusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "\n",
    "print(\"\\nüîπ Classification Report:\")\n",
    "print(classification_report(labels, preds))\n",
    "\n",
    "def save_complete_model(model, tokenizer, severity_mapping, save_path):\n",
    "    model.base_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    torch.save({\n",
    "        'classifier_state': model.classifier.state_dict(),\n",
    "        'num_labels': model.classifier.out_features\n",
    "    }, f\"{save_path}/classifier_state.pt\")\n",
    "    with open(f\"{save_path}/severity_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(severity_mapping, f)\n",
    "\n",
    "save_complete_model(trainer.model, tokenizer, severity_mapping, \"XLM-RoBERTa-Severity\")\n",
    "\n",
    "def load_complete_model(model_path):\n",
    "    base_model = AutoModel.from_pretrained(model_path)\n",
    "    classifier_state = torch.load(f\"{model_path}/classifier_state.pt\", map_location=torch.device('cpu'))\n",
    "    model = CustomModel(base_model, classifier_state['num_labels'])\n",
    "    model.classifier.load_state_dict(classifier_state['classifier_state'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"XLM-RoBERTa-Severity\")\n",
    "model2 = load_complete_model(\"XLM-RoBERTa-Severity\")\n",
    "print(\"‚úÖ Model Loaded Successfully!\")\n",
    "\n",
    "def predict_severity(text, model, tokenizer, severity_mapping):\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    logits = outputs[\"logits\"]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    severity_mapping_reverse = {v: k for k, v in severity_mapping.items()}\n",
    "    return severity_mapping_reverse[predicted_label]\n",
    "\n",
    "test_samples = [\"ÿßŸÑÿßÿπÿ±ÿßÿ∂ ÿ¥ÿØŸäÿØŸá\", \"ÿ™ÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸâ ÿ±ÿßÿ≠Ÿá ŸÅŸÇÿ∑\", \"ÿ≠ÿßŸÑŸá ŸÖÿ™Ÿàÿ≥ÿ∑Ÿá\", \"ŸÜÿ≤ŸäŸÅ ÿ≠ÿßÿØ\"]\n",
    "for text in test_samples:\n",
    "    predicted_severity = predict_severity(text, model2, tokenizer, severity_mapping)\n",
    "    print(f\"\\nüîπ Input: {text}\")\n",
    "    print(f\"Predicted Severity: {predicted_severity}\")\n",
    "\n",
    "shutil.make_archive(\"XLM-RoBERTa-Severity\", 'zip', \"XLM-RoBERTa-Severity\")\n",
    "print(\"\\n‚úÖ Model Saved & Zipped for Download!\")\n",
    "\n",
    "# Download link\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'XLM-RoBERTa-Severity.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T21:31:46.503020Z",
     "iopub.status.busy": "2025-06-16T21:31:46.502378Z",
     "iopub.status.idle": "2025-06-16T21:31:46.598509Z",
     "shell.execute_reply": "2025-06-16T21:31:46.597896Z",
     "shell.execute_reply.started": "2025-06-16T21:31:46.502996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Classification Report (high precision):\n",
      "Accuracy: 0.8539\n",
      "Macro Avg F1-score: 0.8143\n",
      "Weighted Avg F1-score: 0.8582\n",
      "\n",
      "üîπ Full Report Dictionary:\n",
      "{'0': {'f1-score': 0.9001151657871668,\n",
      "       'precision': 0.9317130453348517,\n",
      "       'recall': 0.870590191188695,\n",
      "       'support': 30075},\n",
      " '1': {'f1-score': 0.7283944846926852,\n",
      "       'precision': 0.6668948990071893,\n",
      "       'recall': 0.8023890433528987,\n",
      "       'support': 9711},\n",
      " 'accuracy': 0.8539435982506409,\n",
      " 'macro avg': {'f1-score': 0.814254825239926,\n",
      "               'precision': 0.7993039721710204,\n",
      "               'recall': 0.8364896172707968,\n",
      "               'support': 39786},\n",
      " 'weighted avg': {'f1-score': 0.8582014389961218,\n",
      "                  'precision': 0.8670760117303695,\n",
      "                  'recall': 0.8539435982506409,\n",
      "                  'support': 39786}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "report = classification_report(labels, preds, output_dict=True)\n",
    "print(\"\\nüîπ Classification Report (high precision):\")\n",
    "print(f\"Accuracy: {accuracy_score(labels, preds):.4f}\")\n",
    "print(f\"Macro Avg F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"Weighted Avg F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "print(\"\\nüîπ Full Report Dictionary:\")\n",
    "pprint(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5394582,
     "sourceId": 8962508,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7671547,
     "sourceId": 12180580,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
