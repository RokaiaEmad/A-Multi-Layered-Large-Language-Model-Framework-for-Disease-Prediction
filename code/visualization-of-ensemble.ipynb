{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11121568,"sourceType":"datasetVersion","datasetId":6935315}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def generate_ensemble_combinations_analysis(results_df):\n    \"\"\"\n    Generate analysis of different ensemble combinations (2, 3, or 4 models)\n    using the existing predictions from individual models\n    \"\"\"\n    print(\"\\nüîπ Analyzing different ensemble combinations...\")\n    \n    # Fix model names (remove brackets and numbers from BioBert)\n    original_model_cols = [f\"{name}_Prediction\" for name in model_names]\n    fixed_model_names = [name.replace(\"AraBert\", \"AraBERT\") for name in model_names]\n    fixed_model_names = [name.replace(\"distilBert\", \"Multilingual DistilBERT\") for name in model_names]\n    fixed_model_names = [name.replace(\"BioBert (2)\", \"BioBert\") for name in model_names]\n    fixed_model_names = [name.replace(\"multiBert\", \"mBERT\") for name in model_names]\n    fixed_model_names = [name.replace(\"xlmRoBERTa\", \"'XLM-RoBERTa\") for name in model_names]\n\n    \n    # Get the ground truth\n    y_true = results_df[\"True_Category\"]\n    \n    # Function to perform soft voting ensemble on a combination of models\n    def soft_voting_ensemble(model_cols):\n        # Get one-hot encodings for each model's predictions\n        encodings = []\n        for col in model_cols:\n            # Get predictions\n            preds = results_df[col].values\n            \n            # Convert to one-hot encoding\n            one_hot = np.zeros((len(preds), len(valid_categories)))\n            for i, pred in enumerate(preds):\n                category_idx = valid_categories.index(pred)\n                one_hot[i, category_idx] = 1\n            \n            encodings.append(one_hot)\n        \n        # Average the one-hot encodings\n        avg_encoding = np.mean(encodings, axis=0)\n        \n        # Get the most probable category for each sample\n        ensemble_preds = [valid_categories[np.argmax(avg_encoding[i])] for i in range(len(avg_encoding))]\n        \n        # Calculate accuracy\n        accuracy = np.mean(np.array(ensemble_preds) == y_true.values)\n        \n        return ensemble_preds, accuracy\n    \n    # =============================================================================\n    # Analyze pairs of models (2-model ensembles)\n    # =============================================================================\n    print(\"Analyzing 2-model combinations...\")\n    \n    # Generate all combinations of 2 models\n    pairs = []\n    pair_accuracies = []\n    pair_names = []\n    \n    from itertools import combinations\n    for combo in combinations(range(len(original_model_cols)), 2):\n        model_cols = [original_model_cols[i] for i in combo]\n        model_names_combo = [fixed_model_names[i] for i in combo]\n        \n        # Get predictions and accuracy for this combination\n        ensemble_preds, accuracy = soft_voting_ensemble(model_cols)\n        \n        # Store results\n        pairs.append((model_cols, ensemble_preds))\n        pair_accuracies.append(accuracy)\n        pair_names.append(\" + \".join(model_names_combo))\n    \n    # Create DataFrame with results\n    pair_results = pd.DataFrame({\n        'Model_Combination': pair_names,\n        'Accuracy': pair_accuracies\n    })\n    \n    # Sort by accuracy (descending)\n    pair_results = pair_results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n    \n    # Save results\n    pair_results.to_csv(\"visualizations/pair_ensemble_results.csv\", index=False)\n    \n    # Plot top pairs\n    plt.figure(figsize=(14, 8))\n    bars = plt.bar(pair_results['Model_Combination'], pair_results['Accuracy'], color='lightblue')\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n    \n    plt.title(\"Accuracy of 2-Model Ensemble Combinations\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Model Combination\", fontsize=14)\n    plt.ylim(pair_results['Accuracy'].min() * 0.98, pair_results['Accuracy'].max() * 1.02)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/pair_ensemble_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # =============================================================================\n    # Analyze triplets of models (3-model ensembles)\n    # =============================================================================\n    print(\"Analyzing 3-model combinations...\")\n    \n    # Generate all combinations of 3 models\n    triplets = []\n    triplet_accuracies = []\n    triplet_names = []\n    \n    for combo in combinations(range(len(original_model_cols)), 3):\n        model_cols = [original_model_cols[i] for i in combo]\n        model_names_combo = [fixed_model_names[i] for i in combo]\n        \n        # Get predictions and accuracy for this combination\n        ensemble_preds, accuracy = soft_voting_ensemble(model_cols)\n        \n        # Store results\n        triplets.append((model_cols, ensemble_preds))\n        triplet_accuracies.append(accuracy)\n        triplet_names.append(\" + \".join(model_names_combo))\n    \n    # Create DataFrame with results\n    triplet_results = pd.DataFrame({\n        'Model_Combination': triplet_names,\n        'Accuracy': triplet_accuracies\n    })\n    \n    # Sort by accuracy (descending)\n    triplet_results = triplet_results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n    \n    # Save results\n    triplet_results.to_csv(\"visualizations/triplet_ensemble_results.csv\", index=False)\n    \n    # Plot top triplets\n    plt.figure(figsize=(14, 8))\n    bars = plt.bar(triplet_results['Model_Combination'], triplet_results['Accuracy'], color='lightgreen')\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n    \n    plt.title(\"Accuracy of 3-Model Ensemble Combinations\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Model Combination\", fontsize=14)\n    plt.ylim(triplet_results['Accuracy'].min() * 0.98, triplet_results['Accuracy'].max() * 1.02)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/triplet_ensemble_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # =============================================================================\n    # Analyze quartets of models (4-model ensembles) if we have at least 4 models\n    # =============================================================================\n    if len(original_model_cols) >= 4:\n        print(\"Analyzing 4-model combinations...\")\n        \n        # Generate all combinations of 4 models\n        quartets = []\n        quartet_accuracies = []\n        quartet_names = []\n        \n        for combo in combinations(range(len(original_model_cols)), 4):\n            model_cols = [original_model_cols[i] for i in combo]\n            model_names_combo = [fixed_model_names[i] for i in combo]\n            \n            # Get predictions and accuracy for this combination\n            ensemble_preds, accuracy = soft_voting_ensemble(model_cols)\n            \n            # Store results\n            quartets.append((model_cols, ensemble_preds))\n            quartet_accuracies.append(accuracy)\n            quartet_names.append(\" + \".join(model_names_combo))\n        \n        # Create DataFrame with results\n        quartet_results = pd.DataFrame({\n            'Model_Combination': quartet_names,\n            'Accuracy': quartet_accuracies\n        })\n        \n        # Sort by accuracy (descending)\n        quartet_results = quartet_results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n        \n        # Save results\n        quartet_results.to_csv(\"visualizations/quartet_ensemble_results.csv\", index=False)\n        \n        # Plot quartets\n        plt.figure(figsize=(14, 8))\n        bars = plt.bar(quartet_results['Model_Combination'], quartet_results['Accuracy'], color='lightsalmon')\n        \n        # Add accuracy values on top of bars\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                    f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n        \n        plt.title(\"Accuracy of 4-Model Ensemble Combinations\", fontsize=16)\n        plt.ylabel(\"Accuracy\", fontsize=14)\n        plt.xlabel(\"Model Combination\", fontsize=14)\n        plt.ylim(quartet_results['Accuracy'].min() * 0.98, quartet_results['Accuracy'].max() * 1.02)\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.xticks(rotation=90)\n        plt.tight_layout()\n        \n        plt.savefig(\"visualizations/quartet_ensemble_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n        plt.close()\n    \n    # =============================================================================\n    # Create a summary comparison chart of best combinations from each size\n    # =============================================================================\n    print(\"Creating ensemble summary comparison...\")\n    \n    # Collect best models from each group\n    best_models = []\n    best_accuracies = []\n    group_names = []\n    \n    # Add individual models\n    individual_accuracies = []\n    for i, model_name in enumerate(fixed_model_names):\n        original_name = model_names[i]\n        accuracy = (results_df[\"True_Category\"] == results_df[f\"{original_name}_Prediction\"]).mean()\n        individual_accuracies.append((model_name, accuracy))\n    \n    best_individual = max(individual_accuracies, key=lambda x: x[1])\n    best_models.append(best_individual[0])\n    best_accuracies.append(best_individual[1])\n    group_names.append(\"Best Single Model\")\n    \n    # Add best pair\n    if len(pair_results) > 0:\n        best_models.append(pair_results.iloc[0]['Model_Combination'])\n        best_accuracies.append(pair_results.iloc[0]['Accuracy'])\n        group_names.append(\"Best 2-Model Ensemble\")\n    \n    # Add best triplet\n    if len(triplet_results) > 0:\n        best_models.append(triplet_results.iloc[0]['Model_Combination'])\n        best_accuracies.append(triplet_results.iloc[0]['Accuracy'])\n        group_names.append(\"Best 3-Model Ensemble\")\n    \n    # Add best quartet\n    if len(original_model_cols) >= 4 and len(quartet_results) > 0:\n        best_models.append(quartet_results.iloc[0]['Model_Combination'])\n        best_accuracies.append(quartet_results.iloc[0]['Accuracy'])\n        group_names.append(\"Best 4-Model Ensemble\")\n    \n    # Add full ensemble\n    full_ensemble_accuracy = (results_df[\"True_Category\"] == results_df[\"Ensemble_Prediction\"]).mean()\n    best_models.append(\"All Models\")\n    best_accuracies.append(full_ensemble_accuracy)\n    group_names.append(f\"Full Ensemble ({len(original_model_cols)} Models)\")\n    \n    # Create summary DataFrame\n    summary_df = pd.DataFrame({\n        'Ensemble_Type': group_names,\n        'Models': best_models,\n        'Accuracy': best_accuracies\n    })\n    \n    # Save summary\n    summary_df.to_csv(\"visualizations/ensemble_summary.csv\", index=False)\n    \n    # Plot summary comparison\n    plt.figure(figsize=(12, 8))\n    \n    # Use a color gradient for the bars\n    colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(group_names)))\n    \n    bars = plt.bar(group_names, best_accuracies, color=colors)\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n    \n    plt.title(\"Accuracy Comparison of Best Ensemble Combinations\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Ensemble Type\", fontsize=14)\n    plt.ylim(min(best_accuracies) * 0.98, max(best_accuracies) * 1.02)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/ensemble_summary_comparison.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # =============================================================================\n    # Create a detailed comparison chart showing performance by category for the best models\n    # =============================================================================\n    print(\"Creating category-wise performance comparison for best ensembles...\")\n    \n    # Fix Arabic category names\n    arabic_categories = {cat: fix_arabic_text(cat) for cat in valid_categories}\n    \n    # Calculate per-category accuracies for each best ensemble\n    category_performance = {}\n    \n    # Initialize category performance dictionary\n    for category in valid_categories:\n        category_performance[arabic_categories[category]] = {}\n    \n    # Function to get predictions for a specific ensemble combination\n    def get_ensemble_predictions(model_combination_str, original_model_cols):\n        if model_combination_str == \"All Models\":\n            return results_df[\"Ensemble_Prediction\"].values\n        \n        # Parse model combination string to get individual models\n        if \" + \" in model_combination_str:\n            model_names_combo = model_combination_str.split(\" + \")\n            model_cols = []\n            \n            for model_name in model_names_combo:\n                # Find the corresponding original column\n                for i, original_name in enumerate(fixed_model_names):\n                    if original_name == model_name:\n                        model_cols.append(original_model_cols[i])\n                        break\n            \n            # Get predictions using soft voting\n            ensemble_preds, _ = soft_voting_ensemble(model_cols)\n            return ensemble_preds\n        else:\n            # It's a single model\n            idx = fixed_model_names.index(model_combination_str)\n            return results_df[original_model_cols[idx]].values\n    \n    # Calculate accuracies by category for each best ensemble\n    for ensemble_type, model_combo in zip(group_names, best_models):\n        ensemble_predictions = get_ensemble_predictions(model_combo, original_model_cols)\n        \n        for category in valid_categories:\n            # Filter for this category\n            category_mask = results_df[\"True_Category\"] == category\n            category_indices = np.where(category_mask)[0]\n            \n            if len(category_indices) > 0:\n                # Extract predictions for this category\n                category_preds = [ensemble_predictions[i] for i in category_indices]\n                \n                # Calculate accuracy\n                correct = sum(pred == category for pred in category_preds)\n                accuracy = correct / len(category_indices)\n            else:\n                accuracy = 0\n                \n            category_performance[arabic_categories[category]][ensemble_type] = accuracy\n    \n    # Convert to DataFrame for easier plotting\n    category_comparison_df = pd.DataFrame(category_performance).T\n    \n    # Save the data\n    category_comparison_df.to_csv(\"visualizations/best_ensembles_category_performance.csv\")\n    \n    # Plot the comparison\n    plt.figure(figsize=(18, 10))\n    category_comparison_df.plot(kind='bar', figsize=(18, 10))\n    plt.title(\"Category Performance of Best Ensemble Combinations\", fontsize=16)\n    plt.xlabel(\"Category\", fontsize=14)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.legend(title=\"Ensemble Type\", loc='upper left', bbox_to_anchor=(1, 1))\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/best_ensembles_category_performance.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    print(\"\\n‚úÖ Ensemble combination analysis completed\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:32:38.268787Z","iopub.execute_input":"2025-03-22T01:32:38.269163Z","iopub.status.idle":"2025-03-22T01:32:38.346532Z","shell.execute_reply.started":"2025-03-22T01:32:38.269129Z","shell.execute_reply":"2025-03-22T01:32:38.345644Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Add these imports at the top of your file\nimport arabic_reshaper\nfrom bidi.algorithm import get_display\n\n# Function to fix Arabic text rendering\ndef fix_arabic_text(text):\n    \"\"\"\n    Reshape Arabic text for proper display in matplotlib\n    \"\"\"\n    reshaped_text = arabic_reshaper.reshape(text)\n    bidi_text = get_display(reshaped_text)\n    return bidi_text\n\n# Updated visualization functions with Arabic text support\ndef generate_visualizations(results_df):\n    \"\"\"\n    Generate and save visualizations for research paper with Arabic text support\n    \n    Parameters:\n    - results_df: DataFrame containing all predictions\n    \"\"\"\n    print(\"\\nüîπ Generating visualizations for research paper...\")\n    \n    # Setup for better Arabic text display\n    plt.rcParams['font.family'] = 'Arial'\n    \n    # Fix model names (remove brackets and numbers from BioBert)\n    fixed_model_names = [name.replace(\"BioBert (2)\", \"BioBert\") for name in model_names]\n    \n    # Update results_df column names if BioBert (2) exists\n    if \"BioBert (2)_Prediction\" in results_df.columns:\n        results_df = results_df.rename(columns={\"BioBert (2)_Prediction\": \"BioBert_Prediction\"})\n    \n    # 1. Generate confusion matrices\n    generate_confusion_matrices(results_df, fixed_model_names)\n    \n    # 2. Generate accuracy comparison bar chart\n    generate_accuracy_comparison(results_df, fixed_model_names)\n    \n    # 3. Generate per-category performance chart\n    generate_category_performance(results_df, fixed_model_names)\n    \n    # 4. Generate model agreement heatmap\n    generate_model_agreement_heatmap(results_df, fixed_model_names)\n    \n    # 5. Generate ensemble improvement chart\n    generate_ensemble_improvement_chart(results_df, fixed_model_names)\n    \n    # 6. Generate ensemble combinations analysis\n    generate_ensemble_combinations_analysis(results_df)\n    \n    print(\"\\n‚úÖ All visualizations generated and saved in 'visualizations' directory\")\n\ndef generate_confusion_matrices(results_df, fixed_model_names):\n    \"\"\"Generate confusion matrices for each model and the ensemble with Arabic text support\"\"\"\n    # Fix Arabic category names\n    arabic_categories = [fix_arabic_text(cat) for cat in valid_categories]\n    \n    plt.figure(figsize=(20, 16))\n    \n    # Create subplots for each model + ensemble\n    models_to_plot = fixed_model_names + [\"Ensemble\"]\n    num_models = len(models_to_plot)\n    rows = (num_models + 1) // 2\n    \n    for i, model_name in enumerate(models_to_plot):\n        plt.subplot(rows, 2, i+1)\n        \n        if model_name == \"Ensemble\":\n            y_true = results_df[\"True_Category\"]\n            y_pred = results_df[\"Ensemble_Prediction\"]\n        else:\n            y_true = results_df[\"True_Category\"]\n            original_name = model_names[fixed_model_names.index(model_name)]\n            y_pred = results_df[f\"{original_name}_Prediction\"]\n            \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred, labels=valid_categories)\n        \n        # Plot confusion matrix\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=arabic_categories, yticklabels=arabic_categories)\n        plt.title(f\"{model_name} Confusion Matrix\")\n        plt.xlabel(\"Predicted Category\")\n        plt.ylabel(\"True Category\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.yticks(rotation=45)\n        plt.tight_layout()\n    \n    plt.savefig(\"visualizations/confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # Also save individual high-res confusion matrices\n    for model_name in models_to_plot:\n        plt.figure(figsize=(12, 10))\n        \n        if model_name == \"Ensemble\":\n            y_true = results_df[\"True_Category\"]\n            y_pred = results_df[\"Ensemble_Prediction\"]\n        else:\n            y_true = results_df[\"True_Category\"]\n            original_name = model_names[fixed_model_names.index(model_name)]\n            y_pred = results_df[f\"{original_name}_Prediction\"]\n            \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred, labels=valid_categories)\n        \n        # Plot confusion matrix\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=arabic_categories, yticklabels=arabic_categories)\n        plt.title(f\"{model_name} Confusion Matrix\")\n        plt.xlabel(\"Predicted Category\")\n        plt.ylabel(\"True Category\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.yticks(rotation=45)\n        plt.tight_layout()\n        \n        plt.savefig(f\"visualizations/confusion_matrix_{model_name}.png\", dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\ndef generate_accuracy_comparison(results_df, fixed_model_names):\n    \"\"\"Generate bar chart comparing accuracy of all models\"\"\"\n    # Calculate accuracies\n    accuracies = []\n    model_labels = []\n    \n    # Add individual model accuracies\n    for i, model_name in enumerate(fixed_model_names):\n        original_name = model_names[i]\n        accuracy = (results_df[\"True_Category\"] == results_df[f\"{original_name}_Prediction\"]).mean()\n        accuracies.append(accuracy)\n        model_labels.append(model_name)\n    \n    # Add ensemble accuracy\n    ensemble_accuracy = (results_df[\"True_Category\"] == results_df[\"Ensemble_Prediction\"]).mean()\n    accuracies.append(ensemble_accuracy)\n    model_labels.append(\"Ensemble\")\n    \n    # Create bar chart\n    plt.figure(figsize=(12, 8))\n    bars = plt.bar(model_labels, accuracies, color=['skyblue']*len(fixed_model_names) + ['darkblue'])\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n    \n    plt.title(\"Model Accuracy Comparison\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Model\", fontsize=14)\n    plt.ylim(0, max(accuracies) * 1.15)  # Add some space above bars\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/accuracy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\ndef generate_category_performance(results_df, fixed_model_names):\n    \"\"\"Generate per-category performance chart for all models\"\"\"\n    # Fix Arabic category names\n    arabic_categories = {cat: fix_arabic_text(cat) for cat in valid_categories}\n    \n    # Calculate per-category accuracies for each model\n    category_performance = {}\n    \n    # Initialize category performance dictionary\n    for category in valid_categories:\n        category_performance[arabic_categories[category]] = {}\n    \n    # Calculate accuracies by category for each model\n    all_models = fixed_model_names + [\"Ensemble\"]\n    for i, model_name in enumerate(all_models):\n        for category in valid_categories:\n            # Filter for this category\n            category_mask = results_df[\"True_Category\"] == category\n            \n            if model_name == \"Ensemble\":\n                correct_predictions = results_df.loc[category_mask, \"Ensemble_Prediction\"] == category\n            else:\n                original_name = model_names[fixed_model_names.index(model_name)] if model_name in fixed_model_names else model_name\n                correct_predictions = results_df.loc[category_mask, f\"{original_name}_Prediction\"] == category\n            \n            # Calculate accuracy\n            if sum(category_mask) > 0:\n                accuracy = correct_predictions.sum() / sum(category_mask)\n            else:\n                accuracy = 0\n                \n            category_performance[arabic_categories[category]][model_name] = accuracy\n    \n    # Convert to DataFrame for easier plotting\n    category_df = pd.DataFrame(category_performance).T\n    \n    # Plot\n    plt.figure(figsize=(16, 10))\n    category_df.plot(kind='bar', figsize=(16, 10))\n    plt.title(\"Model Performance by Category\", fontsize=16)\n    plt.xlabel(\"Category\", fontsize=14)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.legend(title=\"Model\", loc='upper left', bbox_to_anchor=(1, 1))\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/category_performance.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # Also save the data\n    category_df.to_csv(\"visualizations/category_performance.csv\")\n\ndef generate_model_agreement_heatmap(results_df, fixed_model_names):\n    \"\"\"Generate heatmap showing agreement between models\"\"\"\n    # Create a dictionary to store agreement percentages\n    agreement_matrix = {}\n    \n    # Include all individual models and ensemble\n    all_models = fixed_model_names + [\"Ensemble\"]\n    \n    # Initialize the matrix\n    for model1 in all_models:\n        agreement_matrix[model1] = {}\n        for model2 in all_models:\n            agreement_matrix[model1][model2] = 0.0\n    \n    # Calculate agreement percentages\n    for model1 in all_models:\n        for model2 in all_models:\n            # Get prediction columns\n            if model1 == \"Ensemble\":\n                pred1 = results_df[\"Ensemble_Prediction\"]\n            else:\n                original_name1 = model_names[fixed_model_names.index(model1)]\n                pred1 = results_df[f\"{original_name1}_Prediction\"]\n                \n            if model2 == \"Ensemble\":\n                pred2 = results_df[\"Ensemble_Prediction\"]\n            else:\n                original_name2 = model_names[fixed_model_names.index(model2)]\n                pred2 = results_df[f\"{original_name2}_Prediction\"]\n            \n            # Calculate agreement percentage\n            agreement = np.mean(pred1 == pred2)\n            agreement_matrix[model1][model2] = agreement\n    \n    # Convert to DataFrame\n    agreement_df = pd.DataFrame(agreement_matrix)\n    \n    # Plot heatmap\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(agreement_df, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", vmin=0, vmax=1)\n    plt.title(\"Model Agreement Heatmap\", fontsize=16)\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/model_agreement.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\ndef generate_ensemble_improvement_chart(results_df, fixed_model_names):\n    \"\"\"Generate chart showing where ensemble improves over individual models\"\"\"\n    # Calculate where ensemble is correct but models are wrong\n    improvements = {}\n    \n    # For each individual model\n    for i, model_name in enumerate(fixed_model_names):\n        original_name = model_names[i]\n        # Cases where ensemble is correct\n        ensemble_correct = results_df[\"True_Category\"] == results_df[\"Ensemble_Prediction\"]\n        \n        # Cases where this model is incorrect\n        model_incorrect = results_df[\"True_Category\"] != results_df[f\"{original_name}_Prediction\"]\n        \n        # Cases where ensemble improves over this model\n        improvement = ensemble_correct & model_incorrect\n        improvements[model_name] = improvement.sum()\n    \n    # Plot improvement chart\n    plt.figure(figsize=(12, 6))\n    plt.bar(improvements.keys(), improvements.values(), color='green')\n    plt.title(\"Number of Cases Where Ensemble Corrects Individual Model Errors\", fontsize=16)\n    plt.ylabel(\"Number of Improvements\", fontsize=14)\n    plt.xlabel(\"Base Model\", fontsize=14)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=45, ha=\"right\")\n    \n    # Add counts on top of bars\n    for i, (model, count) in enumerate(improvements.items()):\n        plt.text(i, count + 5, str(count), ha='center', fontweight='bold')\n    \n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/ensemble_improvements.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # Also calculate the percentage of total samples where ensemble corrects models\n    total_samples = len(results_df)\n    improvement_percentages = {model: count/total_samples*100 for model, count in improvements.items()}\n    \n    # Save these stats\n    improvement_df = pd.DataFrame({\n        'Model': list(improvements.keys()),\n        'Improvement_Count': list(improvements.values()),\n        'Improvement_Percentage': [improvement_percentages[model] for model in improvements.keys()]\n    })\n    improvement_df.to_csv(\"visualizations/ensemble_improvement_stats.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:32:55.694896Z","iopub.execute_input":"2025-03-22T01:32:55.695204Z","iopub.status.idle":"2025-03-22T01:32:55.798568Z","shell.execute_reply.started":"2025-03-22T01:32:55.695181Z","shell.execute_reply":"2025-03-22T01:32:55.797934Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pip install arabic-reshaper\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:32:48.868341Z","iopub.execute_input":"2025-03-22T01:32:48.868624Z","iopub.status.idle":"2025-03-22T01:32:53.293855Z","shell.execute_reply.started":"2025-03-22T01:32:48.868602Z","shell.execute_reply":"2025-03-22T01:32:53.292713Z"}},"outputs":[{"name":"stdout","text":"Collecting arabic-reshaper\n  Downloading arabic_reshaper-3.0.0-py3-none-any.whl.metadata (12 kB)\nDownloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\nInstalling collected packages: arabic-reshaper\nSuccessfully installed arabic-reshaper-3.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Main script to run all the analyses on the saved ensemble predictions\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport os\nimport arabic_reshaper\nfrom bidi.algorithm import get_display\nfrom itertools import combinations\n\n# Create directory for visualizations\nos.makedirs(\"visualizations\", exist_ok=True)\n\n# Define valid medical categories\nvalid_categories = [\n    \"ÿßŸÖÿ±ÿßÿ∂ ŸÜÿ≥ÿßÿ¶Ÿäÿ©\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπÿ∂ŸÑÿßÿ™ ŸàÿßŸÑÿπÿ∏ÿßŸÖ Ÿà ÿßŸÑŸÖŸÅÿßÿµŸÑ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨Ÿáÿßÿ≤ ÿßŸÑŸáÿ∂ŸÖŸä\",\n    \"ÿßŸÑÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©\",\n    \"ÿ∑ÿ® ÿßŸÑÿßÿ≥ŸÜÿßŸÜ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑŸÇŸÑÿ® Ÿà ÿßŸÑÿ¥ÿ±ÿßŸäŸäŸÜ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπŸäŸàŸÜ\",\n    \"ÿßŸÜŸÅ ÿßÿ∞ŸÜ Ÿàÿ≠ŸÜÿ¨ÿ±ÿ©\",\n    \"ÿ¨ÿ±ÿßÿ≠ÿ© ÿ™ÿ¨ŸÖŸäŸÑ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿØŸÖ\"\n]\n\n# Original model names from your code\nmodel_names = [\n    \"AraBert\",\n    \"BioBert (2)\",\n    \"distilBert\",\n    \"multiBert\",\n    \"xlmRoBERTaa\"\n]\n\n# Function to fix Arabic text rendering\ndef fix_arabic_text(text):\n    \"\"\"\n    Reshape Arabic text for proper display in matplotlib\n    \"\"\"\n    reshaped_text = arabic_reshaper.reshape(text)\n    bidi_text = get_display(reshaped_text)\n    return bidi_text\n\n# Import the visualization functions\n# Note: These would be imported from the previous artifacts\n# For brevity, I'm assuming those functions are available\n\ndef run_ensemble_analysis():\n    \"\"\"\n    Main function to run all analyses on the saved ensemble predictions\n    \"\"\"\n    print(\"üîπ Starting ensemble analysis on saved predictions...\")\n    \n    # Load the saved predictions\n    try:\n        results_df = pd.read_csv(\"/kaggle/input/ensembleee/ensemble_individual_prediction.csv\")\n        print(f\"‚úÖ Loaded predictions data with {len(results_df)} samples.\")\n    except FileNotFoundError:\n        print(\"‚ùå Error: The file 'ensemble_individual_prediction.csv' was not found.\")\n        print(\"Please make sure you've run the ensemble prediction code first.\")\n        return\n    \n    # Check if the data has the expected format\n    required_columns = [\"Text\", \"True_Category\", \"Ensemble_Prediction\"]\n    for model in model_names:\n        required_columns.append(f\"{model}_Prediction\")\n    \n    missing_columns = [col for col in required_columns if col not in results_df.columns]\n    \n    if missing_columns:\n        print(f\"‚ùå Error: The following required columns are missing from the data: {missing_columns}\")\n        return\n    \n    # Fix model names (remove brackets and numbers from BioBert)\n    fixed_model_names = [name.replace(\"BioBert (2)\", \"BioBert\") for name in model_names]\n    \n    # Update results_df column names if BioBert (2) exists\n    if \"BioBert (2)_Prediction\" in results_df.columns:\n        results_df = results_df.rename(columns={\"BioBert (2)_Prediction\": \"BioBert_Prediction\"})\n    \n    # Print data summary\n    print(\"\\nüîπ Dataset Summary:\")\n    print(f\"Total samples: {len(results_df)}\")\n    print(\"\\nCategory distribution:\")\n    category_counts = results_df[\"True_Category\"].value_counts()\n    for category, count in category_counts.items():\n        print(f\"  - {category}: {count} samples ({count/len(results_df)*100:.2f}%)\")\n    \n    # Calculate individual model accuracies\n    print(\"\\nüîπ Individual Model Accuracies:\")\n    for i, model_name in enumerate(model_names):\n        accuracy = (results_df[\"True_Category\"] == results_df[f\"{model_name}_Prediction\"]).mean()\n        print(f\"  - {fixed_model_names[i]}: {accuracy:.4f}\")\n    \n    # Calculate ensemble accuracy\n    ensemble_accuracy = (results_df[\"True_Category\"] == results_df[\"Ensemble_Prediction\"]).mean()\n    print(f\"  - Ensemble (All Models): {ensemble_accuracy:.4f}\")\n    \n    # Generate all visualizations\n    print(\"\\nüîπ Generating all visualizations...\")\n    \n    # Generate confusion matrices\n    generate_confusion_matrices(results_df, fixed_model_names)\n    \n    # Generate accuracy comparison chart\n    generate_accuracy_comparison(results_df, fixed_model_names)\n    \n    # Generate per-category performance chart\n    generate_category_performance(results_df, fixed_model_names)\n    \n    # Generate model agreement heatmap\n    generate_model_agreement_heatmap(results_df, fixed_model_names)\n    \n    # Generate ensemble improvement chart\n    generate_ensemble_improvement_chart(results_df, fixed_model_names)\n    \n    # Generate ensemble combinations analysis\n    generate_ensemble_combinations_analysis(results_df)\n    \n    print(\"\\n‚úÖ Analysis complete! All visualizations saved to the 'visualizations' directory.\")\n\n# Run the analysis if this script is executed directly\n# Run the analysis if this script is executed directly\nif __name__ == \"__main__\":\n    run_ensemble_analysis()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:34:25.695591Z","iopub.execute_input":"2025-03-22T01:34:25.695929Z","iopub.status.idle":"2025-03-22T01:34:26.014670Z","shell.execute_reply.started":"2025-03-22T01:34:25.695902Z","shell.execute_reply":"2025-03-22T01:34:26.013597Z"}},"outputs":[{"name":"stdout","text":"üîπ Starting ensemble analysis on saved predictions...\n‚úÖ Loaded predictions data with 39789 samples.\n\nüîπ Dataset Summary:\nTotal samples: 39789\n\nCategory distribution:\n  - ÿßŸÖÿ±ÿßÿ∂ ŸÜÿ≥ÿßÿ¶Ÿäÿ©: 14032 samples (35.27%)\n  - ÿßŸÜŸÅ ÿßÿ∞ŸÜ Ÿàÿ≠ŸÜÿ¨ÿ±ÿ©: 3912 samples (9.83%)\n  - ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπÿ∂ŸÑÿßÿ™ ŸàÿßŸÑÿπÿ∏ÿßŸÖ Ÿà ÿßŸÑŸÖŸÅÿßÿµŸÑ: 3712 samples (9.33%)\n  - ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπŸäŸàŸÜ: 3660 samples (9.20%)\n  - ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑŸÇŸÑÿ® Ÿà ÿßŸÑÿ¥ÿ±ÿßŸäŸäŸÜ: 3190 samples (8.02%)\n  - ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨Ÿáÿßÿ≤ ÿßŸÑŸáÿ∂ŸÖŸä: 3177 samples (7.98%)\n  - ÿßŸÑÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©: 2219 samples (5.58%)\n  - ÿ∑ÿ® ÿßŸÑÿßÿ≥ŸÜÿßŸÜ: 2202 samples (5.53%)\n  - ÿ¨ÿ±ÿßÿ≠ÿ© ÿ™ÿ¨ŸÖŸäŸÑ: 1969 samples (4.95%)\n  - ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿØŸÖ: 1716 samples (4.31%)\n\nüîπ Individual Model Accuracies:\n  - AraBert: 0.9140\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'BioBert (2)_Prediction'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-4031cd7c4299>\u001b[0m in \u001b[0;36m<cell line: 128>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Run the analysis if this script is executed directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mrun_ensemble_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-4031cd7c4299>\u001b[0m in \u001b[0;36mrun_ensemble_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüîπ Individual Model Accuracies:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"True_Category\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{model_name}_Prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - {fixed_model_names[i]}: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'BioBert (2)_Prediction'"],"ename":"KeyError","evalue":"'BioBert (2)_Prediction'","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# Main script to run all the analyses on the saved ensemble predictions\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport os\nimport arabic_reshaper\nfrom bidi.algorithm import get_display\nfrom itertools import combinations\n\n# Create directory for visualizations\nos.makedirs(\"visualizations\", exist_ok=True)\n\n# Define valid medical categories\nvalid_categories = [\n    \"ÿßŸÖÿ±ÿßÿ∂ ŸÜÿ≥ÿßÿ¶Ÿäÿ©\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπÿ∂ŸÑÿßÿ™ ŸàÿßŸÑÿπÿ∏ÿßŸÖ Ÿà ÿßŸÑŸÖŸÅÿßÿµŸÑ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨Ÿáÿßÿ≤ ÿßŸÑŸáÿ∂ŸÖŸä\",\n    \"ÿßŸÑÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿ¨ŸÜÿ≥Ÿäÿ©\",\n    \"ÿ∑ÿ® ÿßŸÑÿßÿ≥ŸÜÿßŸÜ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑŸÇŸÑÿ® Ÿà ÿßŸÑÿ¥ÿ±ÿßŸäŸäŸÜ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿπŸäŸàŸÜ\",\n    \"ÿßŸÜŸÅ ÿßÿ∞ŸÜ Ÿàÿ≠ŸÜÿ¨ÿ±ÿ©\",\n    \"ÿ¨ÿ±ÿßÿ≠ÿ© ÿ™ÿ¨ŸÖŸäŸÑ\",\n    \"ÿßŸÖÿ±ÿßÿ∂ ÿßŸÑÿØŸÖ\"\n]\n\n# Original model names from your code\nmodel_names = [\n    \"AraBert\",\n    \"BioBert (2)\",  # Removing the \"(2)\" here to match the expected column names in the DataFrame\n    \"distilBert\",\n    \"multiBert\",\n    \"xlmRoBERTaa\"  # Fixed typo in \"xlmRoBERTaa\"\n]\n\n# Function to fix Arabic text rendering\ndef fix_arabic_text(text):\n    \"\"\"\n    Reshape Arabic text for proper display in matplotlib\n    \"\"\"\n    reshaped_text = arabic_reshaper.reshape(text)\n    bidi_text = get_display(reshaped_text)\n    return bidi_text\n\n# Define all the visualization functions\ndef generate_confusion_matrices(results_df, fixed_model_names):\n    \"\"\"Generate confusion matrices for each model and the ensemble with Arabic text support\"\"\"\n    # Fix Arabic category names\n    arabic_categories = [fix_arabic_text(cat) for cat in valid_categories]\n    \n    plt.figure(figsize=(20, 16))\n    \n    # Create subplots for each model + ensemble\n    models_to_plot = fixed_model_names + [\"Ensemble\"]\n    num_models = len(models_to_plot)\n    rows = (num_models + 1) // 2\n    \n    for i, model_name in enumerate(models_to_plot):\n        plt.subplot(rows, 2, i+1)\n        \n        if model_name == \"Ensemble\":\n            y_true = results_df[\"True_Category\"]\n            y_pred = results_df[\"Ensemble_Prediction\"]\n        else:\n            y_true = results_df[\"True_Category\"]\n            original_name = model_names[fixed_model_names.index(model_name)]\n            y_pred = results_df[f\"{original_name}_Prediction\"]\n            \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred, labels=valid_categories)\n        \n        # Plot confusion matrix\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=arabic_categories, yticklabels=arabic_categories)\n        plt.title(f\"{model_name} Confusion Matrix\")\n        plt.xlabel(\"Predicted Category\")\n        plt.ylabel(\"True Category\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.yticks(rotation=45)\n        plt.tight_layout()\n    \n    plt.savefig(\"visualizations/confusion_matrices.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # Also save individual high-res confusion matrices\n    for model_name in models_to_plot:\n        plt.figure(figsize=(12, 10))\n        \n        if model_name == \"Ensemble\":\n            y_true = results_df[\"True_Category\"]\n            y_pred = results_df[\"Ensemble_Prediction\"]\n        else:\n            y_true = results_df[\"True_Category\"]\n            original_name = model_names[fixed_model_names.index(model_name)]\n            y_pred = results_df[f\"{original_name}_Prediction\"]\n            \n        # Calculate confusion matrix\n        cm = confusion_matrix(y_true, y_pred, labels=valid_categories)\n        \n        # Plot confusion matrix\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=arabic_categories, yticklabels=arabic_categories)\n        plt.title(f\"{model_name} Confusion Matrix\")\n        plt.xlabel(\"Predicted Category\")\n        plt.ylabel(\"True Category\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.yticks(rotation=45)\n        plt.tight_layout()\n        \n        plt.savefig(f\"visualizations/confusion_matrix_{model_name}.png\", dpi=300, bbox_inches=\"tight\")\n        plt.close()\n\ndef generate_accuracy_comparison(results_df, fixed_model_names):\n    \"\"\"Generate bar chart comparing accuracy of all models\"\"\"\n    # Calculate accuracies\n    accuracies = []\n    model_labels = []\n    \n    # Add individual model accuracies\n    for i, model_name in enumerate(fixed_model_names):\n        original_name = model_names[i]\n        accuracy = (results_df[\"True_Category\"] == results_df[f\"{original_name}_Prediction\"]).mean()\n        accuracies.append(accuracy)\n        model_labels.append(model_name)\n    \n    # Add ensemble accuracy\n    ensemble_accuracy = (results_df[\"True_Category\"] == results_df[\"Ensemble_Prediction\"]).mean()\n    accuracies.append(ensemble_accuracy)\n    model_labels.append(\"Ensemble\")\n    \n    # Create bar chart\n    plt.figure(figsize=(12, 8))\n    bars = plt.bar(model_labels, accuracies, color=['skyblue']*len(fixed_model_names) + ['darkblue'])\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n    \n    plt.title(\"Model Accuracy Comparison\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Model\", fontsize=14)\n    plt.ylim(0, max(accuracies) * 1.15)  # Add some space above bars\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/accuracy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\ndef generate_category_performance(results_df, fixed_model_names):\n    \"\"\"Generate per-category performance chart for all models\"\"\"\n    # Fix Arabic category names\n    arabic_categories = {cat: fix_arabic_text(cat) for cat in valid_categories}\n    \n    # Calculate per-category accuracies for each model\n    category_performance = {}\n    \n    # Initialize category performance dictionary\n    for category in valid_categories:\n        category_performance[arabic_categories[category]] = {}\n    \n    # Calculate accuracies by category for each model\n    all_models = fixed_model_names + [\"Ensemble\"]\n    for i, model_name in enumerate(all_models):\n        for category in valid_categories:\n            # Filter for this category\n            category_mask = results_df[\"True_Category\"] == category\n            \n            if model_name == \"Ensemble\":\n                correct_predictions = results_df.loc[category_mask, \"Ensemble_Prediction\"] == category\n            else:\n                original_name = model_names[fixed_model_names.index(model_name)] if model_name in fixed_model_names else model_name\n                correct_predictions = results_df.loc[category_mask, f\"{original_name}_Prediction\"] == category\n            \n            # Calculate accuracy\n            if sum(category_mask) > 0:\n                accuracy = correct_predictions.sum() / sum(category_mask)\n            else:\n                accuracy = 0\n                \n            category_performance[arabic_categories[category]][model_name] = accuracy\n    \n    # Convert to DataFrame for easier plotting\n    category_df = pd.DataFrame(category_performance).T\n    \n    # Plot\n    plt.figure(figsize=(16, 10))\n    category_df.plot(kind='bar', figsize=(16, 10))\n    plt.title(\"Model Performance by Category\", fontsize=16)\n    plt.xlabel(\"Category\", fontsize=14)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.legend(title=\"Model\", loc='upper left', bbox_to_anchor=(1, 1))\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/category_performance.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # Also save the data\n    category_df.to_csv(\"visualizations/category_performance.csv\")\n\ndef generate_model_agreement_heatmap(results_df, fixed_model_names):\n    \"\"\"Generate heatmap showing agreement between models\"\"\"\n    # Create a dictionary to store agreement percentages\n    agreement_matrix = {}\n    \n    # Include all individual models and ensemble\n    all_models = fixed_model_names + [\"Ensemble\"]\n    \n    # Initialize the matrix\n    for model1 in all_models:\n        agreement_matrix[model1] = {}\n        for model2 in all_models:\n            agreement_matrix[model1][model2] = 0.0\n    \n    # Calculate agreement percentages\n    for model1 in all_models:\n        for model2 in all_models:\n            # Get prediction columns\n            if model1 == \"Ensemble\":\n                pred1 = results_df[\"Ensemble_Prediction\"]\n            else:\n                original_name1 = model_names[fixed_model_names.index(model1)]\n                pred1 = results_df[f\"{original_name1}_Prediction\"]\n                \n            if model2 == \"Ensemble\":\n                pred2 = results_df[\"Ensemble_Prediction\"]\n            else:\n                original_name2 = model_names[fixed_model_names.index(model2)]\n                pred2 = results_df[f\"{original_name2}_Prediction\"]\n            \n            # Calculate agreement percentage\n            agreement = np.mean(pred1 == pred2)\n            agreement_matrix[model1][model2] = agreement\n    \n    # Convert to DataFrame\n    agreement_df = pd.DataFrame(agreement_matrix)\n    \n    # Plot heatmap\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(agreement_df, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", vmin=0, vmax=1)\n    plt.title(\"Model Agreement Heatmap\", fontsize=16)\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/model_agreement.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\ndef generate_ensemble_improvement_chart(results_df, fixed_model_names):\n    \"\"\"Generate chart showing where ensemble improves over individual models\"\"\"\n    # Calculate where ensemble is correct but models are wrong\n    improvements = {}\n    \n    # For each individual model\n    for i, model_name in enumerate(fixed_model_names):\n        original_name = model_names[i]\n        # Cases where ensemble is correct\n        ensemble_correct = results_df[\"True_Category\"] == results_df[\"Ensemble_Prediction\"]\n        \n        # Cases where this model is incorrect\n        model_incorrect = results_df[\"True_Category\"] != results_df[f\"{original_name}_Prediction\"]\n        \n        # Cases where ensemble improves over this model\n        improvement = ensemble_correct & model_incorrect\n        improvements[model_name] = improvement.sum()\n    \n    # Plot improvement chart\n    plt.figure(figsize=(12, 6))\n    plt.bar(improvements.keys(), improvements.values(), color='green')\n    plt.title(\"Number of Cases Where Ensemble Corrects Individual Model Errors\", fontsize=16)\n    plt.ylabel(\"Number of Improvements\", fontsize=14)\n    plt.xlabel(\"Base Model\", fontsize=14)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=45, ha=\"right\")\n    \n    # Add counts on top of bars\n    for i, (model, count) in enumerate(improvements.items()):\n        plt.text(i, count + 5, str(count), ha='center', fontweight='bold')\n    \n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/ensemble_improvements.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # Also calculate the percentage of total samples where ensemble corrects models\n    total_samples = len(results_df)\n    improvement_percentages = {model: count/total_samples*100 for model, count in improvements.items()}\n    \n    # Save these stats\n    improvement_df = pd.DataFrame({\n        'Model': list(improvements.keys()),\n        'Improvement_Count': list(improvements.values()),\n        'Improvement_Percentage': [improvement_percentages[model] for model in improvements.keys()]\n    })\n    improvement_df.to_csv(\"visualizations/ensemble_improvement_stats.csv\", index=False)\n\ndef generate_ensemble_combinations_analysis(results_df):\n    \"\"\"\n    Generate analysis of different ensemble combinations (2, 3, or 4 models)\n    using the existing predictions from individual models\n    \"\"\"\n    print(\"\\nüîπ Analyzing different ensemble combinations...\")\n    \n    # Fix model names (remove brackets and numbers from BioBert)\n    original_model_cols = [f\"{name}_Prediction\" for name in model_names]\n    fixed_model_names = [name.replace(\"AraBert\", \"AraBERT\") for name in model_names]\n    fixed_model_names = [name.replace(\"distilBert\", \"Multilingual DistilBERT\") for name in fixed_model_names]\n    fixed_model_names = [name.replace(\"BioBert (2)\", \"BioBERT\") for name in fixed_model_names]\n    fixed_model_names = [name.replace(\"multiBert\", \"mBERT\") for name in fixed_model_names]\n    fixed_model_names = [name.replace(\"xlmRoBERTaa\", \"XLM-RoBERTa\") for name in fixed_model_names]\n    \n    # Get the ground truth\n    y_true = results_df[\"True_Category\"]\n    \n    # Function to perform soft voting ensemble on a combination of models\n    def soft_voting_ensemble(model_cols):\n        # Get one-hot encodings for each model's predictions\n        encodings = []\n        for col in model_cols:\n            # Get predictions\n            preds = results_df[col].values\n            \n            # Convert to one-hot encoding\n            one_hot = np.zeros((len(preds), len(valid_categories)))\n            for i, pred in enumerate(preds):\n                category_idx = valid_categories.index(pred)\n                one_hot[i, category_idx] = 1\n            \n            encodings.append(one_hot)\n        \n        # Average the one-hot encodings\n        avg_encoding = np.mean(encodings, axis=0)\n        \n        # Get the most probable category for each sample\n        ensemble_preds = [valid_categories[np.argmax(avg_encoding[i])] for i in range(len(avg_encoding))]\n        \n        # Calculate accuracy\n        accuracy = np.mean(np.array(ensemble_preds) == y_true.values)\n        \n        return ensemble_preds, accuracy\n    \n    # =============================================================================\n    # Analyze pairs of models (2-model ensembles)\n    # =============================================================================\n    print(\"Analyzing 2-model combinations...\")\n    \n    # Generate all combinations of 2 models\n    pairs = []\n    pair_accuracies = []\n    pair_names = []\n    \n    from itertools import combinations\n    for combo in combinations(range(len(original_model_cols)), 2):\n        model_cols = [original_model_cols[i] for i in combo]\n        model_names_combo = [fixed_model_names[i] for i in combo]\n        \n        # Get predictions and accuracy for this combination\n        ensemble_preds, accuracy = soft_voting_ensemble(model_cols)\n        \n        # Store results\n        pairs.append((model_cols, ensemble_preds))\n        pair_accuracies.append(accuracy)\n        pair_names.append(\" + \".join(model_names_combo))\n    \n    # Create DataFrame with results\n    pair_results = pd.DataFrame({\n        'Model_Combination': pair_names,\n        'Accuracy': pair_accuracies\n    })\n    \n    # Sort by accuracy (descending)\n    pair_results = pair_results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n    \n    # Save results\n    pair_results.to_csv(\"visualizations/pair_ensemble_results.csv\", index=False)\n    \n    # Plot top pairs\n    plt.figure(figsize=(14, 8))\n    bars = plt.bar(pair_results['Model_Combination'], pair_results['Accuracy'], color='lightblue')\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n    \n    plt.title(\"Accuracy of 2-Model Ensemble Combinations\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Model Combination\", fontsize=14)\n    plt.ylim(pair_results['Accuracy'].min() * 0.98, pair_results['Accuracy'].max() * 1.02)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/pair_ensemble_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # =============================================================================\n    # Analyze triplets of models (3-model ensembles)\n    # =============================================================================\n    print(\"Analyzing 3-model combinations...\")\n    \n    # Generate all combinations of 3 models\n    triplets = []\n    triplet_accuracies = []\n    triplet_names = []\n    \n    for combo in combinations(range(len(original_model_cols)), 3):\n        model_cols = [original_model_cols[i] for i in combo]\n        model_names_combo = [fixed_model_names[i] for i in combo]\n        \n        # Get predictions and accuracy for this combination\n        ensemble_preds, accuracy = soft_voting_ensemble(model_cols)\n        \n        # Store results\n        triplets.append((model_cols, ensemble_preds))\n        triplet_accuracies.append(accuracy)\n        triplet_names.append(\" + \".join(model_names_combo))\n    \n    # Create DataFrame with results\n    triplet_results = pd.DataFrame({\n        'Model_Combination': triplet_names,\n        'Accuracy': triplet_accuracies\n    })\n    \n    # Sort by accuracy (descending)\n    triplet_results = triplet_results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n    \n    # Save results\n    triplet_results.to_csv(\"visualizations/triplet_ensemble_results.csv\", index=False)\n    \n    # Plot top triplets\n    plt.figure(figsize=(14, 8))\n    bars = plt.bar(triplet_results['Model_Combination'], triplet_results['Accuracy'], color='lightgreen')\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n    \n    plt.title(\"Accuracy of 3-Model Ensemble Combinations\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Model Combination\", fontsize=14)\n    plt.ylim(triplet_results['Accuracy'].min() * 0.98, triplet_results['Accuracy'].max() * 1.02)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/triplet_ensemble_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # =============================================================================\n    # Analyze quartets of models (4-model ensembles) if we have at least 4 models\n    # =============================================================================\n    if len(original_model_cols) >= 4:\n        print(\"Analyzing 4-model combinations...\")\n        \n        # Generate all combinations of 4 models\n        quartets = []\n        quartet_accuracies = []\n        quartet_names = []\n        \n        for combo in combinations(range(len(original_model_cols)), 4):\n            model_cols = [original_model_cols[i] for i in combo]\n            model_names_combo = [fixed_model_names[i] for i in combo]\n            \n            # Get predictions and accuracy for this combination\n            ensemble_preds, accuracy = soft_voting_ensemble(model_cols)\n            \n            # Store results\n            quartets.append((model_cols, ensemble_preds))\n            quartet_accuracies.append(accuracy)\n            quartet_names.append(\" + \".join(model_names_combo))\n        \n        # Create DataFrame with results\n        quartet_results = pd.DataFrame({\n            'Model_Combination': quartet_names,\n            'Accuracy': quartet_accuracies\n        })\n        \n        # Sort by accuracy (descending)\n        quartet_results = quartet_results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n        \n        # Save results\n        quartet_results.to_csv(\"visualizations/quartet_ensemble_results.csv\", index=False)\n        \n        # Plot quartets\n        plt.figure(figsize=(14, 8))\n        bars = plt.bar(quartet_results['Model_Combination'], quartet_results['Accuracy'], color='lightsalmon')\n        \n        # Add accuracy values on top of bars\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                    f'{height:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n        \n        plt.title(\"Accuracy of 4-Model Ensemble Combinations\", fontsize=16)\n        plt.ylabel(\"Accuracy\", fontsize=14)\n        plt.xlabel(\"Model Combination\", fontsize=14)\n        plt.ylim(quartet_results['Accuracy'].min() * 0.98, quartet_results['Accuracy'].max() * 1.02)\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.xticks(rotation=90)\n        plt.tight_layout()\n        \n        plt.savefig(\"visualizations/quartet_ensemble_accuracy.png\", dpi=300, bbox_inches=\"tight\")\n        plt.close()\n    \n    # =============================================================================\n    # Create a summary comparison chart of best combinations from each size\n    # =============================================================================\n    print(\"Creating ensemble summary comparison...\")\n    \n    # Collect best models from each group\n    best_models = []\n    best_accuracies = []\n    group_names = []\n    \n    # Add individual models\n    individual_accuracies = []\n    for i, model_name in enumerate(fixed_model_names):\n        original_name = model_names[i]\n        accuracy = (results_df[\"True_Category\"] == results_df[f\"{original_name}_Prediction\"]).mean()\n        individual_accuracies.append((model_name, accuracy))\n    \n    best_individual = max(individual_accuracies, key=lambda x: x[1])\n    best_models.append(best_individual[0])\n    best_accuracies.append(best_individual[1])\n    group_names.append(\"Best Single Model\")\n    \n    # Add best pair\n    if len(pair_results) > 0:\n        best_models.append(pair_results.iloc[0]['Model_Combination'])\n        best_accuracies.append(pair_results.iloc[0]['Accuracy'])\n        group_names.append(\"Best 2-Model Ensemble\")\n    \n    # Add best triplet\n    if len(triplet_results) > 0:\n        best_models.append(triplet_results.iloc[0]['Model_Combination'])\n        best_accuracies.append(triplet_results.iloc[0]['Accuracy'])\n        group_names.append(\"Best 3-Model Ensemble\")\n    \n    # Add best quartet\n    if len(original_model_cols) >= 4 and 'quartet_results' in locals():\n        best_models.append(quartet_results.iloc[0]['Model_Combination'])\n        best_accuracies.append(quartet_results.iloc[0]['Accuracy'])\n        group_names.append(\"Best 4-Model Ensemble\")\n    \n    # Add full ensemble\n    full_ensemble_accuracy = (results_df[\"True_Category\"] == results_df[\"Ensemble_Prediction\"]).mean()\n    best_models.append(\"All Models\")\n    best_accuracies.append(full_ensemble_accuracy)\n    group_names.append(f\"Full Ensemble ({len(original_model_cols)} Models)\")\n    \n    # Create summary DataFrame\n    summary_df = pd.DataFrame({\n        'Ensemble_Type': group_names,\n        'Models': best_models,\n        'Accuracy': best_accuracies\n    })\n    \n    # Save summary\n    summary_df.to_csv(\"visualizations/ensemble_summary.csv\", index=False)\n    \n    # Plot summary comparison\n    plt.figure(figsize=(12, 8))\n    \n    # Use a color gradient for the bars\n    colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(group_names)))\n    \n    bars = plt.bar(group_names, best_accuracies, color=colors)\n    \n    # Add accuracy values on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n    \n    plt.title(\"Accuracy Comparison of Best Ensemble Combinations\", fontsize=16)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.xlabel(\"Ensemble Type\", fontsize=14)\n    plt.ylim(min(best_accuracies) * 0.98, max(best_accuracies) * 1.02)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/ensemble_summary_comparison.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    # =============================================================================\n    # Create a detailed comparison chart showing performance by category for the best models\n    # =============================================================================\n    print(\"Creating category-wise performance comparison for best ensembles...\")\n    \n    # Fix Arabic category names\n    arabic_categories = {cat: fix_arabic_text(cat) for cat in valid_categories}\n    \n    # Calculate per-category accuracies for each best ensemble\n    category_performance = {}\n    \n    # Initialize category performance dictionary\n    for category in valid_categories:\n        category_performance[arabic_categories[category]] = {}\n    \n    # Function to get predictions for a specific ensemble combination\n    def get_ensemble_predictions(model_combination_str, original_model_cols):\n        if model_combination_str == \"All Models\":\n            return results_df[\"Ensemble_Prediction\"].values\n        \n        # Parse model combination string to get individual models\n        if \" + \" in model_combination_str:\n            model_names_combo = model_combination_str.split(\" + \")\n            model_cols = []\n            \n            for model_name in model_names_combo:\n                # Find the corresponding original column\n                for i, original_name in enumerate(fixed_model_names):\n                    if original_name == model_name:\n                        model_cols.append(original_model_cols[i])\n                        break\n            \n            # Get predictions using soft voting\n            ensemble_preds, _ = soft_voting_ensemble(model_cols)\n            return ensemble_preds\n        else:\n            # It's a single model\n            idx = fixed_model_names.index(model_combination_str)\n            return results_df[original_model_cols[idx]].values\n    \n    # Calculate accuracies by category for each best ensemble\n    for ensemble_type, model_combo in zip(group_names, best_models):\n        ensemble_predictions = get_ensemble_predictions(model_combo, original_model_cols)\n        \n        for category in valid_categories:\n            # Filter for this category\n            category_mask = results_df[\"True_Category\"] == category\n            category_indices = np.where(category_mask)[0]\n            \n            if len(category_indices) > 0:\n                # Extract predictions for this category\n                category_preds = [ensemble_predictions[i] for i in category_indices]\n                \n                # Calculate accuracy\n                correct = sum(pred == category for pred in category_preds)\n                accuracy = correct / len(category_indices)\n            else:\n                accuracy = 0\n                \n            category_performance[arabic_categories[category]][ensemble_type] = accuracy\n    \n    # Convert to DataFrame for easier plotting\n    category_comparison_df = pd.DataFrame(category_performance).T\n    \n    # Save the data\n    category_comparison_df.to_csv(\"visualizations/best_ensembles_category_performance.csv\")\n    \n    # Plot the comparison\n    plt.figure(figsize=(18, 10))\n    category_comparison_df.plot(kind='bar', figsize=(18, 10))\n    plt.title(\"Category Performance of Best Ensemble Combinations\", fontsize=16)\n    plt.xlabel(\"Category\", fontsize=14)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.legend(title=\"Ensemble Type\", loc='upper left', bbox_to_anchor=(1, 1))\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    \n    plt.savefig(\"visualizations/best_ensembles_category_performance.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\n    \n    print(\"\\n‚úÖ Ensemble combination analysis completed\")\n\ndef run_ensemble_analysis():\n    \"\"\"\n    Main function to run all analyses on the saved ensemble predictions\n    \"\"\"\n    print(\"üîπ Starting ensemble analysis on saved predictions...\")\n    \n    # Load the saved predictions\n    try:\n        results_df = pd.read_csv(\"/kaggle/input/ensembleee/ensemble_individual_prediction.csv\")\n        print(f\"‚úÖ Loaded predictions data with {len(results_df)} samples.\")\n    except FileNotFoundError:\n        print(\"‚ùå Error: The file 'ensemble_individual_prediction.csv' was not found.\")\n        print(\"Please make sure you've run the ensemble prediction code first.\")\n        return\n    \n    # Check if the data has the expected columns\n    expected_cols = [\"True_Category\", \"Ensemble_Prediction\"]\n    model_cols = [f\"{name}_Prediction\" for name in model_names]\n    \n    missing_cols = [col for col in expected_cols + model_cols if col not in results_df.columns]\n    if missing_cols:\n        print(f\"‚ùå Error: The following expected columns are missing: {missing_cols}\")\n        return\n    \n    # Fix model names for display purposes\n    fixed_model_names = [name.replace(\"AraBert\", \"AraBERT\") for name in model_names]\n    fixed_model_names = [name.replace(\"distilBert\", \"Multilingual DistilBERT\") for name in fixed_model_names]\n    fixed_model_names = [name.replace(\"BioBert (2)\", \"BioBERT\") for name in fixed_model_names]\n    fixed_model_names = [name.replace(\"multiBert\", \"mBERT\") for name in fixed_model_names]\n    fixed_model_names = [name.replace(\"xlmRoBERTaa\", \"XLM-RoBERTa\") for name in fixed_model_names]\n    \n    print(\"\\nüîπ Generating confusion matrices...\")\n    generate_confusion_matrices(results_df, fixed_model_names)\n    \n    print(\"\\nüîπ Generating accuracy comparison...\")\n    generate_accuracy_comparison(results_df, fixed_model_names)\n    \n    print(\"\\nüîπ Generating category performance analysis...\")\n    generate_category_performance(results_df, fixed_model_names)\n    \n    print(\"\\nüîπ Generating model agreement heatmap...\")\n    generate_model_agreement_heatmap(results_df, fixed_model_names)\n    \n    print(\"\\nüîπ Analyzing ensemble improvements...\")\n    generate_ensemble_improvement_chart(results_df, fixed_model_names)\n    \n    print(\"\\nüîπ Analyzing different ensemble combinations...\")\n    generate_ensemble_combinations_analysis(results_df)\n    \n    print(\"\\n‚úÖ All analyses completed successfully!\")\n    print(\"üìä Visualizations saved in the 'visualizations' directory.\")\n    \n    # Return the results DataFrame in case it's needed for further analysis\n    return results_df\n\n# Execute the analysis if this script is run directly\nif __name__ == \"__main__\":\n    run_ensemble_analysis()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:53:17.337853Z","iopub.execute_input":"2025-03-22T01:53:17.338204Z","iopub.status.idle":"2025-03-22T01:53:46.106014Z","shell.execute_reply.started":"2025-03-22T01:53:17.338176Z","shell.execute_reply":"2025-03-22T01:53:46.105175Z"}},"outputs":[{"name":"stdout","text":"üîπ Starting ensemble analysis on saved predictions...\n‚úÖ Loaded predictions data with 39789 samples.\n\nüîπ Generating confusion matrices...\n\nüîπ Generating accuracy comparison...\n\nüîπ Generating category performance analysis...\n\nüîπ Generating model agreement heatmap...\n\nüîπ Analyzing ensemble improvements...\n\nüîπ Analyzing different ensemble combinations...\n\nüîπ Analyzing different ensemble combinations...\nAnalyzing 2-model combinations...\nAnalyzing 3-model combinations...\nAnalyzing 4-model combinations...\nCreating ensemble summary comparison...\nCreating category-wise performance comparison for best ensembles...\n\n‚úÖ Ensemble combination analysis completed\n\n‚úÖ All analyses completed successfully!\nüìä Visualizations saved in the 'visualizations' directory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x1000 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1800x1000 with 0 Axes>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import shutil\n    print(\"\\nüîπ Compressing visualizations directory...\")\n    try:\n        shutil.make_archive(\"visualizations_archive\", \"zip\", \"visualizations\")\n        print(\"‚úÖ Visualizations compressed to 'visualizations_archive.zip'\")\n    except Exception as e:\n        print(f\"‚ùå Error compressing visualizations: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:55:54.874911Z","iopub.execute_input":"2025-03-22T01:55:54.875274Z","iopub.status.idle":"2025-03-22T01:55:54.880885Z","shell.execute_reply.started":"2025-03-22T01:55:54.875244Z","shell.execute_reply":"2025-03-22T01:55:54.879781Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-1449c162ecd2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(\"\\nüîπ Compressing visualizations directory...\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"],"ename":"IndentationError","evalue":"unexpected indent (<ipython-input-13-1449c162ecd2>, line 2)","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"# Add this at the appropriate place in your run_ensemble_analysis function\nprint(\"\\n‚úÖ All analyses completed successfully!\")\nprint(\"üìä Visualizations saved in the 'visualizations' directory.\")\n\n# Compress visualizations directory\nimport shutil\nprint(\"\\nüîπ Compressing visualizations directory...\")\ntry:\n    shutil.make_archive(\"visualizations_archive\", \"zip\", \"visualizations\")\n    print(\"‚úÖ Visualizations compressed to 'visualizations_archive.zip'\")\nexcept Exception as e:\n    print(f\"‚ùå Error compressing visualizations: {str(e)}\")\n\n# Return the results DataFrame in case it's needed for further analysis\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T01:58:01.649040Z","iopub.execute_input":"2025-03-22T01:58:01.649369Z","iopub.status.idle":"2025-03-22T01:58:01.987582Z","shell.execute_reply.started":"2025-03-22T01:58:01.649344Z","shell.execute_reply":"2025-03-22T01:58:01.986912Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ All analyses completed successfully!\nüìä Visualizations saved in the 'visualizations' directory.\n\nüîπ Compressing visualizations directory...\n‚úÖ Visualizations compressed to 'visualizations_archive.zip'\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}