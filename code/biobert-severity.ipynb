{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8962508,"sourceType":"datasetVersion","datasetId":5394582},{"sourceId":12180580,"sourceType":"datasetVersion","datasetId":7671547}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key=\"c26df6b59bfb128917e73bbb00a79ca7e9324a11\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T19:23:01.077608Z","iopub.execute_input":"2025-06-16T19:23:01.077931Z","iopub.status.idle":"2025-06-16T19:23:10.196437Z","shell.execute_reply.started":"2025-06-16T19:23:01.077898Z","shell.execute_reply":"2025-06-16T19:23:10.195906Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkokyloka2003\u001b[0m (\u001b[33mkokyloka2003-msa\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport logging\nimport shutil\nimport os\n\n# Enable logging\nlogging.basicConfig(level=logging.INFO)\n\n# Load full dataset\ntrain_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Train.xlsx\"\ntest_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Test.xlsx\"\ntrain_df = pd.read_excel(train_path)\ntest_df = pd.read_excel(test_path)\nall_data = pd.concat([train_df, test_df], ignore_index=True)\nall_data = all_data[['q_body', 'category', 'severity']]\n\n# Valid categories and severity levels\nvalid_categories = [\n    \"Ø§Ù…Ø±Ø§Ø¶ Ù†Ø³Ø§Ø¦ÙŠØ©\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹Ø¶Ù„Ø§Øª ÙˆØ§Ù„Ø¹Ø¸Ø§Ù… Ùˆ Ø§Ù„Ù…ÙØ§ØµÙ„\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ\",\n    \"Ø§Ù„Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù†Ø³ÙŠØ©\",\n    \"Ø·Ø¨ Ø§Ù„Ø§Ø³Ù†Ø§Ù†\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨ Ùˆ Ø§Ù„Ø´Ø±Ø§ÙŠÙŠÙ†\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹ÙŠÙˆÙ†\",\n    \"Ø§Ù†Ù Ø§Ø°Ù† ÙˆØ­Ù†Ø¬Ø±Ø©\",\n    \"Ø¬Ø±Ø§Ø­Ø© ØªØ¬Ù…ÙŠÙ„\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¯Ù…\"\n]\nvalid_severity = [\"Ø­Ø±Ø¬\", \"ØºÙŠØ± Ø­Ø±Ø¬\"]\n\n# Filter dataset\nall_data = all_data[all_data[\"category\"].isin(valid_categories) & all_data[\"severity\"].isin(valid_severity)]\nall_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Split into train and test\ntrain_df, test_df = train_test_split(all_data, test_size=0.2, random_state=42, stratify=all_data[\"severity\"])\n\n# Map severity to labels\nseverity_mapping = {sev: i for i, sev in enumerate(valid_severity)}\ntrain_df['label'] = train_df['severity'].map(severity_mapping)\ntest_df['label'] = test_df['severity'].map(severity_mapping)\n\n# Load model & tokenizer\nmodel_name = \"monologg/biobert_v1.1_pubmed\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Tokenization\ndef tokenize_function(examples):\n    return tokenizer(examples[\"q_body\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_dataset = Dataset.from_pandas(train_df[['q_body', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['q_body', 'label']])\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Define custom model\nbase_model = AutoModel.from_pretrained(model_name)\n\nclass CustomModel(torch.nn.Module):\n    def __init__(self, base_model, num_labels):\n        super(CustomModel, self).__init__()\n        self.base_model = base_model\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        loss = self.loss_fn(logits, labels) if labels is not None else None\n        return {\"loss\": loss, \"logits\": logits}\n\nmodel = CustomModel(base_model, num_labels=len(valid_severity))\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    per_device_train_batch_size=96,\n    per_device_eval_batch_size=96,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\"\n)\n\n# Metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n    acc = accuracy_score(labels, predictions)\n    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Train\nprint(\"\\nğŸ”¹ Starting Training...\")\ntrainer.train()\n\n# Evaluate\ntest_metrics = trainer.evaluate(test_dataset)\nprint(\"\\nğŸ”¹ Test Metrics:\", test_metrics)\n\n# Predict\npredictions = trainer.predict(test_dataset)\npreds = np.argmax(predictions.predictions, axis=1)\nlabels = predictions.label_ids\n\nprint(\"\\nğŸ”¹ Confusion Matrix:\")\nprint(confusion_matrix(labels, preds))\nprint(\"\\nğŸ”¹ Classification Report:\")\nprint(classification_report(labels, preds))\n\n# Save model\ndef save_complete_model(model, tokenizer, severity_mapping, save_path):\n    model.base_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    torch.save({\n        'classifier_state': model.classifier.state_dict(),\n        'num_labels': model.classifier.out_features\n    }, f\"{save_path}/classifier_state.pt\")\n    with open(f\"{save_path}/severity_mapping.pkl\", \"wb\") as f:\n        pickle.dump(severity_mapping, f)\n\nsave_complete_model(trainer.model, tokenizer, severity_mapping, \"BioBERT-Severity\")\n\n# Load model\ndef load_complete_model(model_path):\n    base_model = AutoModel.from_pretrained(model_path)\n    classifier_state = torch.load(f\"{model_path}/classifier_state.pt\", map_location=torch.device('cpu'))\n    model = CustomModel(base_model, classifier_state['num_labels'])\n    model.classifier.load_state_dict(classifier_state['classifier_state'])\n    model.eval()\n    return model\n\ntokenizer = AutoTokenizer.from_pretrained(\"BioBERT-Severity\")\nmodel2 = load_complete_model(\"BioBERT-Severity\")\nprint(\"âœ… Model Loaded Successfully!\")\n\n# Prediction function\ndef predict_severity(text, model, tokenizer, severity_mapping):\n    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n    logits = outputs[\"logits\"]\n    predicted_label = torch.argmax(logits, dim=-1).item()\n    severity_mapping_reverse = {v: k for k, v in severity_mapping.items()}\n    return severity_mapping_reverse[predicted_label]\n\n# Test predictions\ntest_samples = [\"Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\", \"ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\", \"Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\", \"Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\"]\nfor text in test_samples:\n    predicted_severity = predict_severity(text, model2, tokenizer, severity_mapping)\n    print(f\"\\nğŸ”¹ Input: {text}\")\n    print(f\"Predicted Severity: {predicted_severity}\")\n\n# Zip model\nshutil.make_archive(\"BioBERT-Severity\", 'zip', \"BioBERT-Severity\")\nprint(\"\\nâœ… Model Saved & Zipped for Download!\")\n\n# Download link\nfrom IPython.display import FileLink\nFileLink(r'BioBERT-Severity.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T19:23:41.484766Z","iopub.execute_input":"2025-06-16T19:23:41.485361Z","iopub.status.idle":"2025-06-17T01:15:47.255020Z","shell.execute_reply.started":"2025-06-16T19:23:41.485338Z","shell.execute_reply":"2025-06-17T01:15:47.254265Z"}},"outputs":[{"name":"stderr","text":"2025-06-16 19:23:54.991729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750101835.184806      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750101835.239666      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d8397e3e93f4f1eb4f9879618ee1f30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c500457cb9d42beb9623ab3f8d003af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9395fa7d2cd548f991033e4ee49d3412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3397ce5ac494dca93518673106be8cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/159143 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"017ecfe2d10b4b9a83eb4e1cac5b5d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39786 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5bb13c3b9844eca7bb8a851cc521ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c4530969874d6a9b386c98f479ca7d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/1080694047.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ”¹ Starting Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250616_192523-wq6hj655</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kokyloka2003-msa/huggingface/runs/wq6hj655' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/kokyloka2003-msa/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kokyloka2003-msa/huggingface' target=\"_blank\">https://wandb.ai/kokyloka2003-msa/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kokyloka2003-msa/huggingface/runs/wq6hj655' target=\"_blank\">https://wandb.ai/kokyloka2003-msa/huggingface/runs/wq6hj655</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8290' max='8290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8290/8290 5:43:55, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.470000</td>\n      <td>0.451472</td>\n      <td>0.775976</td>\n      <td>0.763870</td>\n      <td>0.775976</td>\n      <td>0.768305</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.408300</td>\n      <td>0.426025</td>\n      <td>0.792540</td>\n      <td>0.789184</td>\n      <td>0.792540</td>\n      <td>0.790733</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.415600</td>\n      <td>0.444580</td>\n      <td>0.779420</td>\n      <td>0.808212</td>\n      <td>0.779420</td>\n      <td>0.788790</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.374300</td>\n      <td>0.393957</td>\n      <td>0.820339</td>\n      <td>0.812617</td>\n      <td>0.820339</td>\n      <td>0.815086</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.345400</td>\n      <td>0.435303</td>\n      <td>0.792465</td>\n      <td>0.823257</td>\n      <td>0.792465</td>\n      <td>0.801813</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.332000</td>\n      <td>0.392340</td>\n      <td>0.816468</td>\n      <td>0.825127</td>\n      <td>0.816468</td>\n      <td>0.819971</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.326300</td>\n      <td>0.408911</td>\n      <td>0.813201</td>\n      <td>0.830062</td>\n      <td>0.813201</td>\n      <td>0.819083</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.284400</td>\n      <td>0.404631</td>\n      <td>0.820992</td>\n      <td>0.829916</td>\n      <td>0.820992</td>\n      <td>0.824545</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.261700</td>\n      <td>0.412394</td>\n      <td>0.824235</td>\n      <td>0.832113</td>\n      <td>0.824235</td>\n      <td>0.827422</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.274600</td>\n      <td>0.417337</td>\n      <td>0.820967</td>\n      <td>0.832632</td>\n      <td>0.820967</td>\n      <td>0.825351</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ”¹ Test Metrics: {'eval_loss': 0.4123944044113159, 'eval_accuracy': 0.8242346554064244, 'eval_precision': 0.8321127313546847, 'eval_recall': 0.8242346554064244, 'eval_f1': 0.8274218586640452, 'eval_runtime': 177.3038, 'eval_samples_per_second': 224.395, 'eval_steps_per_second': 1.173, 'epoch': 10.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ”¹ Confusion Matrix:\n[[26006  4069]\n [ 2924  6787]]\n\nğŸ”¹ Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.86      0.88     30075\n           1       0.63      0.70      0.66      9711\n\n    accuracy                           0.82     39786\n   macro avg       0.76      0.78      0.77     39786\nweighted avg       0.83      0.82      0.83     39786\n\nâœ… Model Loaded Successfully!\n\nğŸ”¹ Input: Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\nPredicted Severity: Ø­Ø±Ø¬\n\nâœ… Model Saved & Zipped for Download!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/BioBERT-Severity.zip","text/html":"<a href='BioBERT-Severity.zip' target='_blank'>BioBERT-Severity.zip</a><br>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from pprint import pprint\nreport = classification_report(labels, preds, output_dict=True)\nprint(\"\\nğŸ”¹ Classification Report (high precision):\")\nprint(f\"Accuracy: {accuracy_score(labels, preds):.4f}\")\nprint(f\"Macro Avg F1-score: {report['macro avg']['f1-score']:.4f}\")\nprint(f\"Weighted Avg F1-score: {report['weighted avg']['f1-score']:.4f}\")\nprint(\"\\nğŸ”¹ Full Report Dictionary:\")\npprint(report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T01:18:39.836121Z","iopub.execute_input":"2025-06-17T01:18:39.836792Z","iopub.status.idle":"2025-06-17T01:18:39.965175Z","shell.execute_reply.started":"2025-06-17T01:18:39.836767Z","shell.execute_reply":"2025-06-17T01:18:39.964446Z"}},"outputs":[{"name":"stdout","text":"\nğŸ”¹ Classification Report (high precision):\nAccuracy: 0.8242\nMacro Avg F1-score: 0.7707\nWeighted Avg F1-score: 0.8274\n\nğŸ”¹ Full Report Dictionary:\n{'0': {'f1-score': 0.8814846199474621,\n       'precision': 0.8989284479778776,\n       'recall': 0.8647049044056525,\n       'support': 30075},\n '1': {'f1-score': 0.6599893032527836,\n       'precision': 0.6251842299189388,\n       'recall': 0.6988981567294821,\n       'support': 9711},\n 'accuracy': 0.8242346554064244,\n 'macro avg': {'f1-score': 0.7707369616001228,\n               'precision': 0.7620563389484083,\n               'recall': 0.7818015305675673,\n               'support': 39786},\n 'weighted avg': {'f1-score': 0.8274218586640452,\n                  'precision': 0.8321127313546847,\n                  'recall': 0.8242346554064244,\n                  'support': 39786}}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}