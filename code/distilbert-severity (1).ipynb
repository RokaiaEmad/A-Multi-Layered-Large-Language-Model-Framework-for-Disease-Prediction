{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:13:05.137341Z",
     "iopub.status.busy": "2025-06-16T14:13:05.137070Z",
     "iopub.status.idle": "2025-06-16T14:13:05.144449Z",
     "shell.execute_reply": "2025-06-16T14:13:05.143771Z",
     "shell.execute_reply.started": "2025-06-16T14:13:05.137311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"c26df6b59bfb128917e73bbb00a79ca7e9324a11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T15:11:56.786061Z",
     "iopub.status.busy": "2025-06-16T15:11:56.785310Z",
     "iopub.status.idle": "2025-06-16T18:08:51.128513Z",
     "shell.execute_reply": "2025-06-16T18:08:51.127769Z",
     "shell.execute_reply.started": "2025-06-16T15:11:56.786037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9587e66cbad34e4abe223aaffeefab62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea193b88c70747939e2aceaf9fedeb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39786 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/529349445.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8290' max='8290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8290/8290 2:52:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.381113</td>\n",
       "      <td>0.819736</td>\n",
       "      <td>0.834027</td>\n",
       "      <td>0.819736</td>\n",
       "      <td>0.824854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.358262</td>\n",
       "      <td>0.834841</td>\n",
       "      <td>0.846345</td>\n",
       "      <td>0.834841</td>\n",
       "      <td>0.839002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.385458</td>\n",
       "      <td>0.825240</td>\n",
       "      <td>0.853672</td>\n",
       "      <td>0.825240</td>\n",
       "      <td>0.833083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.339605</td>\n",
       "      <td>0.855150</td>\n",
       "      <td>0.858010</td>\n",
       "      <td>0.855150</td>\n",
       "      <td>0.856409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.427503</td>\n",
       "      <td>0.831775</td>\n",
       "      <td>0.857073</td>\n",
       "      <td>0.831775</td>\n",
       "      <td>0.838899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.405774</td>\n",
       "      <td>0.852813</td>\n",
       "      <td>0.859562</td>\n",
       "      <td>0.852813</td>\n",
       "      <td>0.855424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.500175</td>\n",
       "      <td>0.833534</td>\n",
       "      <td>0.857061</td>\n",
       "      <td>0.833534</td>\n",
       "      <td>0.840307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.516724</td>\n",
       "      <td>0.840170</td>\n",
       "      <td>0.859249</td>\n",
       "      <td>0.840170</td>\n",
       "      <td>0.845951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>0.527261</td>\n",
       "      <td>0.848590</td>\n",
       "      <td>0.859921</td>\n",
       "      <td>0.848590</td>\n",
       "      <td>0.852513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.571391</td>\n",
       "      <td>0.845121</td>\n",
       "      <td>0.859920</td>\n",
       "      <td>0.845121</td>\n",
       "      <td>0.849913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Test Metrics: {'eval_loss': 0.33960527181625366, 'eval_accuracy': 0.8551500527823858, 'eval_precision': 0.8580098920731865, 'eval_recall': 0.8551500527823858, 'eval_f1': 0.8564093127047725, 'eval_runtime': 89.3446, 'eval_samples_per_second': 445.31, 'eval_steps_per_second': 2.328, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Confusion Matrix:\n",
      "[[26933  3142]\n",
      " [ 2621  7090]]\n",
      "\n",
      "ğŸ”¹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90     30075\n",
      "           1       0.69      0.73      0.71      9711\n",
      "\n",
      "    accuracy                           0.86     39786\n",
      "   macro avg       0.80      0.81      0.81     39786\n",
      "weighted avg       0.86      0.86      0.86     39786\n",
      "\n",
      "âœ… Model Loaded Successfully!\n",
      "\n",
      "ğŸ”¹ Input: Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\n",
      "Predicted Severity: Ø­Ø±Ø¬\n",
      "\n",
      "ğŸ”¹ Input: ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\n",
      "Predicted Severity: Ø­Ø±Ø¬\n",
      "\n",
      "ğŸ”¹ Input: Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\n",
      "Predicted Severity: Ø­Ø±Ø¬\n",
      "\n",
      "ğŸ”¹ Input: Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\n",
      "Predicted Severity: Ø­Ø±Ø¬\n",
      "\n",
      "âœ… Model Saved & Zipped for Download!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='DistilBERT-Severity.zip' target='_blank'>DistilBERT-Severity.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/DistilBERT-Severity.zip"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "train_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Train.xlsx\"\n",
    "test_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Test.xlsx\"\n",
    "train_df = pd.read_excel(train_path)\n",
    "test_df = pd.read_excel(test_path)\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "all_data = all_data[['q_body', 'category', 'severity']]\n",
    "\n",
    "valid_categories = [\n",
    "    \"Ø§Ù…Ø±Ø§Ø¶ Ù†Ø³Ø§Ø¦ÙŠØ©\",\n",
    "    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹Ø¶Ù„Ø§Øª ÙˆØ§Ù„Ø¹Ø¸Ø§Ù… Ùˆ Ø§Ù„Ù…ÙØ§ØµÙ„\",\n",
    "    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ\",\n",
    "    \"Ø§Ù„Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù†Ø³ÙŠØ©\",\n",
    "    \"Ø·Ø¨ Ø§Ù„Ø§Ø³Ù†Ø§Ù†\",\n",
    "    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨ Ùˆ Ø§Ù„Ø´Ø±Ø§ÙŠÙŠÙ†\",\n",
    "    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹ÙŠÙˆÙ†\",\n",
    "    \"Ø§Ù†Ù Ø§Ø°Ù† ÙˆØ­Ù†Ø¬Ø±Ø©\",\n",
    "    \"Ø¬Ø±Ø§Ø­Ø© ØªØ¬Ù…ÙŠÙ„\",\n",
    "    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¯Ù…\"\n",
    "]\n",
    "valid_severity = [\"Ø­Ø±Ø¬\", \"ØºÙŠØ± Ø­Ø±Ø¬\"]\n",
    "\n",
    "all_data = all_data[all_data[\"category\"].isin(valid_categories) & all_data[\"severity\"].isin(valid_severity)]\n",
    "all_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(all_data, test_size=0.2, random_state=42, stratify=all_data[\"severity\"])\n",
    "\n",
    "severity_mapping = {sev: i for i, sev in enumerate(valid_severity)}\n",
    "train_df['label'] = train_df['severity'].map(severity_mapping)\n",
    "test_df['label'] = test_df['severity'].map(severity_mapping)\n",
    "\n",
    "model_name = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"q_body\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['q_body', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['q_body', 'label']])\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = self.loss_fn(logits, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = CustomModel(base_model, num_labels=len(valid_severity))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=96,\n",
    "    per_device_eval_batch_size=96,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=3e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ”¹ Starting Training...\")\n",
    "trainer.train()\n",
    "\n",
    "test_metrics = trainer.evaluate(test_dataset)\n",
    "print(\"\\nğŸ”¹ Test Metrics:\", test_metrics)\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "print(\"\\nğŸ”¹ Confusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "print(\"\\nğŸ”¹ Classification Report:\")\n",
    "print(classification_report(labels, preds))\n",
    "\n",
    "def save_complete_model(model, tokenizer, severity_mapping, save_path):\n",
    "    model.base_model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    torch.save({\n",
    "        'classifier_state': model.classifier.state_dict(),\n",
    "        'num_labels': model.classifier.out_features\n",
    "    }, f\"{save_path}/classifier_state.pt\")\n",
    "    with open(f\"{save_path}/severity_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(severity_mapping, f)\n",
    "\n",
    "save_complete_model(trainer.model, tokenizer, severity_mapping, \"DistilBERT-Severity\")\n",
    "\n",
    "def load_complete_model(model_path):\n",
    "    base_model = AutoModel.from_pretrained(model_path)\n",
    "    classifier_state = torch.load(f\"{model_path}/classifier_state.pt\", map_location=torch.device('cpu'))\n",
    "    model = CustomModel(base_model, classifier_state['num_labels'])\n",
    "    model.classifier.load_state_dict(classifier_state['classifier_state'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DistilBERT-Severity\")\n",
    "model2 = load_complete_model(\"DistilBERT-Severity\")\n",
    "print(\"âœ… Model Loaded Successfully!\")\n",
    "\n",
    "def predict_severity(text, model, tokenizer, severity_mapping):\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    logits = outputs[\"logits\"]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    severity_mapping_reverse = {v: k for k, v in severity_mapping.items()}\n",
    "    return severity_mapping_reverse[predicted_label]\n",
    "\n",
    "test_samples = [\"Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\", \"ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\", \"Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\", \"Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\"]\n",
    "for text in test_samples:\n",
    "    predicted_severity = predict_severity(text, model2, tokenizer, severity_mapping)\n",
    "    print(f\"\\nğŸ”¹ Input: {text}\")\n",
    "    print(f\"Predicted Severity: {predicted_severity}\")\n",
    "\n",
    "shutil.make_archive(\"DistilBERT-Severity\", 'zip', \"DistilBERT-Severity\")\n",
    "print(\"\\nâœ… Model Saved & Zipped for Download!\")\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'DistilBERT-Severity.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5394582,
     "sourceId": 8962508,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7671547,
     "sourceId": 12180580,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
