{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12180580,"sourceType":"datasetVersion","datasetId":7671547}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport logging\nimport shutil\nimport os\n\n# Enable logging for monitoring\nlogging.basicConfig(level=logging.INFO)\n\n# Paths to dataset\ntrain_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Train.xlsx\"\ntest_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Test.xlsx\"\n\n# ğŸ”¹ Define valid severity levels\n\n\ntrain_df = pd.read_excel(train_path)\ntest_df = pd.read_excel(test_path)\n\n# ğŸ”¹ Combine and select only necessary columns\nall_data = pd.concat([train_df, test_df], ignore_index=True)[['q_body', 'severity', 'category']]\n\n# ğŸ”¹ Filter rows by valid category only\nvalid_categories = [\n    \"Ø§Ù…Ø±Ø§Ø¶ Ù†Ø³Ø§Ø¦ÙŠØ©\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹Ø¶Ù„Ø§Øª ÙˆØ§Ù„Ø¹Ø¸Ø§Ù… Ùˆ Ø§Ù„Ù…ÙØ§ØµÙ„\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ\",\n    \"Ø§Ù„Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù†Ø³ÙŠØ©\",\n    \"Ø·Ø¨ Ø§Ù„Ø§Ø³Ù†Ø§Ù†\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨ Ùˆ Ø§Ù„Ø´Ø±Ø§ÙŠÙŠÙ†\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹ÙŠÙˆÙ†\",\n    \"Ø§Ù†Ù Ø§Ø°Ù† ÙˆØ­Ù†Ø¬Ø±Ø©\",\n    \"Ø¬Ø±Ø§Ø­Ø© ØªØ¬Ù…ÙŠÙ„\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¯Ù…\"\n]\nall_data = all_data[all_data[\"category\"].isin(valid_categories)].reset_index(drop=True)\n\n# ğŸ”¹ Define valid severity levels\nvalid_severity = [\"Ø­Ø±Ø¬\", \"ØºÙŠØ± Ø­Ø±Ø¬\"]\nall_data = all_data[all_data[\"severity\"].isin(valid_severity)].reset_index(drop=True)\n\n# ğŸ”¹ Shuffle and stratified split based on severity (not category)\nall_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_df, test_df = train_test_split(\n    all_data[['q_body', 'severity']],  # keep only relevant columns\n    test_size=0.2,\n    random_state=42,\n    stratify=all_data['severity']\n)\n\n# ğŸ”¹ Display distributions (optional)\nprint(\"\\nğŸ”¹ Training Severity Distribution:\")\nprint(train_df[\"severity\"].value_counts())\nprint(\"\\nğŸ”¹ Test Severity Distribution:\")\nprint(test_df[\"severity\"].value_counts())\n\n\n# ğŸ”¹ Map severity levels to numerical labels\nseverity_mapping = {sev: i for i, sev in enumerate(valid_severity)}\ntrain_df['label'] = train_df['severity'].map(severity_mapping)\ntest_df['label'] = test_df['severity'].map(severity_mapping)\n\n# ğŸ”¹ Model and Tokenizer\nmodel_name = \"bert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# ğŸ”¹ Tokenization Function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"q_body\"], padding=\"max_length\", truncation=True, max_length=128)\n\n# ğŸ”¹ Convert Data to Hugging Face Dataset Format\ntrain_dataset = Dataset.from_pandas(train_df[['q_body', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['q_body', 'label']])\n\n# Apply Tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# ğŸ”¹ Load Base Model\nbase_model = AutoModel.from_pretrained(model_name)\n\n# ğŸ”¹ Define Custom Model Class\nclass CustomModel(torch.nn.Module):\n    def __init__(self, base_model, num_labels):\n        super(CustomModel, self).__init__()  # Corrected line\n        self.base_model = base_model\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n        return {\"loss\": loss, \"logits\": logits}\n\n\n# ğŸ”¹ Initialize Model\nmodel = CustomModel(base_model, num_labels=len(valid_severity))\n\n# ğŸ”¹ Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    per_device_train_batch_size=96,\n    per_device_eval_batch_size=96,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\"\n)\n\n# ğŸ”¹ Compute Metrics Function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n    acc = accuracy_score(labels, predictions)\n    return {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\n# ğŸ”¹ Trainer Initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# ğŸ”¹ Start Training\nprint(\"\\nğŸ”¹ Starting Training...\")\ntrainer.train()\n\n# ğŸ”¹ Evaluate Model on Test Dataset\ntest_metrics = trainer.evaluate(test_dataset)\nprint(\"\\nğŸ”¹ Test Metrics:\", test_metrics)\n\n# ğŸ”¹ Get Predictions\npredictions = trainer.predict(test_dataset)\npreds = np.argmax(predictions.predictions, axis=1)\nlabels = predictions.label_ids\n\n# ğŸ”¹ Print Evaluation Metrics\nprint(\"\\nğŸ”¹ Confusion Matrix:\")\nprint(confusion_matrix(labels, preds))\n\nprint(\"\\nğŸ”¹ Classification Report:\")\nprint(classification_report(labels, preds))\n\n# ğŸ”¹ Save Model & Tokenizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:05:03.905489Z","iopub.execute_input":"2025-06-16T15:05:03.906288Z","iopub.status.idle":"2025-06-16T20:45:33.537363Z","shell.execute_reply.started":"2025-06-16T15:05:03.906263Z","shell.execute_reply":"2025-06-16T20:45:33.536371Z"}},"outputs":[{"name":"stdout","text":"\nğŸ”¹ Training Severity Distribution:\nseverity\nØ­Ø±Ø¬        120301\nØºÙŠØ± Ø­Ø±Ø¬     38842\nName: count, dtype: int64\n\nğŸ”¹ Test Severity Distribution:\nseverity\nØ­Ø±Ø¬        30075\nØºÙŠØ± Ø­Ø±Ø¬     9711\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/159143 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad1dfb10eb54751942a8fbe88ec03ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39786 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3903394e9bbd4861a4bbb87f0f6e9e19"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/1033345189.py:142: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ”¹ Starting Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16580' max='16580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16580/16580 5:34:29, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.394600</td>\n      <td>0.383874</td>\n      <td>0.819082</td>\n      <td>0.840206</td>\n      <td>0.819082</td>\n      <td>0.825797</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.339200</td>\n      <td>0.359283</td>\n      <td>0.835017</td>\n      <td>0.849278</td>\n      <td>0.835017</td>\n      <td>0.839874</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.261000</td>\n      <td>0.548598</td>\n      <td>0.773714</td>\n      <td>0.853263</td>\n      <td>0.773714</td>\n      <td>0.788929</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.214200</td>\n      <td>0.413496</td>\n      <td>0.830694</td>\n      <td>0.860213</td>\n      <td>0.830694</td>\n      <td>0.838506</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.125800</td>\n      <td>0.441015</td>\n      <td>0.846831</td>\n      <td>0.860587</td>\n      <td>0.846831</td>\n      <td>0.851358</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.122900</td>\n      <td>0.472137</td>\n      <td>0.848640</td>\n      <td>0.860348</td>\n      <td>0.848640</td>\n      <td>0.852655</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.103000</td>\n      <td>0.672580</td>\n      <td>0.831172</td>\n      <td>0.857696</td>\n      <td>0.831172</td>\n      <td>0.838514</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.063600</td>\n      <td>0.728515</td>\n      <td>0.832981</td>\n      <td>0.859733</td>\n      <td>0.832981</td>\n      <td>0.840299</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.046300</td>\n      <td>0.811524</td>\n      <td>0.835947</td>\n      <td>0.859381</td>\n      <td>0.835947</td>\n      <td>0.842631</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.050600</td>\n      <td>0.886094</td>\n      <td>0.835922</td>\n      <td>0.859563</td>\n      <td>0.835922</td>\n      <td>0.842642</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ”¹ Test Metrics: {'eval_loss': 0.4721367359161377, 'eval_accuracy': 0.8486402252048459, 'eval_precision': 0.8603478544159187, 'eval_recall': 0.8486402252048459, 'eval_f1': 0.8526550288243139, 'eval_runtime': 145.7906, 'eval_samples_per_second': 272.898, 'eval_steps_per_second': 2.847, 'epoch': 10.0}\n\nğŸ”¹ Confusion Matrix:\n[[26182  3893]\n [ 2129  7582]]\n\nğŸ”¹ Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.87      0.90     30075\n           1       0.66      0.78      0.72      9711\n\n    accuracy                           0.85     39786\n   macro avg       0.79      0.83      0.81     39786\nweighted avg       0.86      0.85      0.85     39786\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def save_complete_model(model, tokenizer, severity_mapping, save_path):\n    model.base_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    classifier_state = {\n        'classifier_state': model.classifier.state_dict(),\n        'num_labels': model.classifier.out_features\n    }\n    torch.save(classifier_state, f\"{save_path}/classifier_state.pt\")\n    with open(f\"{save_path}/severity_mapping.pkl\", \"wb\") as f:\n        pickle.dump(severity_mapping, f)\n\n# Save the model\nsave_complete_model(trainer.model, tokenizer, severity_mapping, \"bert-base-multilingual-cased-Severity\")\n\n# ğŸ”¹ Load the Model for Inference\ndef load_complete_model(model_path):\n    base_model = AutoModel.from_pretrained(model_path)\n    classifier_state = torch.load(f\"{model_path}/classifier_state.pt\", map_location=torch.device('cpu'))\n    model = CustomModel(base_model, classifier_state['num_labels'])\n    model.classifier.load_state_dict(classifier_state['classifier_state'])\n    model.eval()\n    return model\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased-Severity\")\nmodel2 = load_complete_model(\"bert-base-multilingual-cased-Severity\")\nprint(\"âœ… Model Loaded Successfully!\")\n\n# ğŸ”¹ Function for Severity Predictions\ndef predict_severity(text, model, tokenizer, severity_mapping):\n    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n    logits = outputs[\"logits\"]\n    predicted_label = torch.argmax(logits, dim=-1).item()\n    severity_mapping_reverse = {v: k for k, v in severity_mapping.items()}\n    return severity_mapping_reverse[predicted_label]\n\n# ğŸ”¹ Sample Severity Predictions\ntest_samples = [\"Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\", \"ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\", \"Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\", \"Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\"]\nfor text in test_samples:\n    predicted_severity = predict_severity(text, model2, tokenizer, severity_mapping)\n    print(f\"\\nğŸ”¹ Input: {text}\")\n    print(f\"Predicted Severity: {predicted_severity}\")\n\n# ğŸ”¹ Zip Model for Download\nshutil.make_archive(\"bert-base-multilingual-cased-Severity\", 'zip', \"bert-base-multilingual-cased-Severity\")\nprint(\"\\nâœ… Model Saved & Zipped for Download!\")\n\n# Provide a download link\nfrom IPython.display import FileLink\nFileLink(r'bert-base-multilingual-cased-Severity.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T20:54:22.847778Z","iopub.execute_input":"2025-06-16T20:54:22.848117Z","iopub.status.idle":"2025-06-16T20:55:05.878224Z","shell.execute_reply.started":"2025-06-16T20:54:22.848094Z","shell.execute_reply":"2025-06-16T20:55:05.877417Z"}},"outputs":[{"name":"stdout","text":"âœ… Model Loaded Successfully!\n\nğŸ”¹ Input: Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\nPredicted Severity: Ø­Ø±Ø¬\n\nâœ… Model Saved & Zipped for Download!\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/bert-base-multilingual-cased-Severity.zip","text/html":"<a href='bert-base-multilingual-cased-Severity.zip' target='_blank'>bert-base-multilingual-cased-Severity.zip</a><br>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"72d0e227429bd347553a5563b7396b82cb04a364\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T21:16:57.371215Z","iopub.execute_input":"2025-06-17T21:16:57.371606Z","iopub.status.idle":"2025-06-17T21:17:06.513233Z","shell.execute_reply.started":"2025-06-17T21:16:57.371585Z","shell.execute_reply":"2025-06-17T21:17:06.512553Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrokaia-emad\u001b[0m (\u001b[33mrokaia-emad-modern-sciences-and-arts-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split  # <-- Add this line\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport logging\nimport shutil\nimport os\n\n\n# Enable logging for monitoring\nlogging.basicConfig(level=logging.INFO)\n\n# Paths to dataset\ntrain_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Train.xlsx\"\ntest_path = \"/kaggle/input/maqa-unbalanced-with-severity/MAQA_Severity_Test.xlsx\"\n\n# ğŸ”¹ Define valid severity levels\n\n\ntrain_df = pd.read_excel(train_path)\ntest_df = pd.read_excel(test_path)\n\n# ğŸ”¹ Combine and select only necessary columns\nall_data = pd.concat([train_df, test_df], ignore_index=True)[['q_body', 'severity', 'category']]\n\n# ğŸ”¹ Filter rows by valid category only\nvalid_categories = [\n    \"Ø§Ù…Ø±Ø§Ø¶ Ù†Ø³Ø§Ø¦ÙŠØ©\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹Ø¶Ù„Ø§Øª ÙˆØ§Ù„Ø¹Ø¸Ø§Ù… Ùˆ Ø§Ù„Ù…ÙØ§ØµÙ„\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ\",\n    \"Ø§Ù„Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù†Ø³ÙŠØ©\",\n    \"Ø·Ø¨ Ø§Ù„Ø§Ø³Ù†Ø§Ù†\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨ Ùˆ Ø§Ù„Ø´Ø±Ø§ÙŠÙŠÙ†\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¹ÙŠÙˆÙ†\",\n    \"Ø§Ù†Ù Ø§Ø°Ù† ÙˆØ­Ù†Ø¬Ø±Ø©\",\n    \"Ø¬Ø±Ø§Ø­Ø© ØªØ¬Ù…ÙŠÙ„\",\n    \"Ø§Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¯Ù…\"\n]\nall_data = all_data[all_data[\"category\"].isin(valid_categories)].reset_index(drop=True)\n\n# ğŸ”¹ Define valid severity levels\nvalid_severity = [\"Ø­Ø±Ø¬\", \"ØºÙŠØ± Ø­Ø±Ø¬\"]\nall_data = all_data[all_data[\"severity\"].isin(valid_severity)].reset_index(drop=True)\n\n# ğŸ”¹ Shuffle and stratified split based on severity (not category)\nall_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_df, test_df = train_test_split(\n    all_data[['q_body', 'severity']],  # keep only relevant columns\n    test_size=0.2,\n    random_state=42,\n    stratify=all_data['severity']\n)\n\n# ğŸ”¹ Display distributions (optional)\nprint(\"\\nğŸ”¹ Training Severity Distribution:\")\nprint(train_df[\"severity\"].value_counts())\nprint(\"\\nğŸ”¹ Test Severity Distribution:\")\nprint(test_df[\"severity\"].value_counts())\n\n\n# ğŸ”¹ Map severity levels to numerical labels\nseverity_mapping = {sev: i for i, sev in enumerate(valid_severity)}\ntrain_df['label'] = train_df['severity'].map(severity_mapping)\ntest_df['label'] = test_df['severity'].map(severity_mapping)\n\n# ğŸ”¹ Model and Tokenizer\nmodel_name = \"aubmindlab/bert-base-arabert\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# ğŸ”¹ Tokenization Function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"q_body\"], padding=\"max_length\", truncation=True, max_length=128)\n\n# ğŸ”¹ Convert Data to Hugging Face Dataset Format\ntrain_dataset = Dataset.from_pandas(train_df[['q_body', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['q_body', 'label']])\n\n# Apply Tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# ğŸ”¹ Load Base Model\nbase_model = AutoModel.from_pretrained(model_name)\n\n# ğŸ”¹ Define Custom Model Class\nclass CustomModel(torch.nn.Module):\n    def __init__(self, base_model, num_labels):\n        super(CustomModel, self).__init__()  # Corrected line\n        self.base_model = base_model\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n        return {\"loss\": loss, \"logits\": logits}\n\n\n# ğŸ”¹ Initialize Model\nmodel = CustomModel(base_model, num_labels=len(valid_severity))\n\n# ğŸ”¹ Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    per_device_train_batch_size=96,\n    per_device_eval_batch_size=96,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    learning_rate=3e-5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\"\n)\n\n# ğŸ”¹ Compute Metrics Function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n    acc = accuracy_score(labels, predictions)\n    return {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\n# ğŸ”¹ Trainer Initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# ğŸ”¹ Start Training\nprint(\"\\nğŸ”¹ Starting Training...\")\ntrainer.train()\n\n# ğŸ”¹ Evaluate Model on Test Dataset\ntest_metrics = trainer.evaluate(test_dataset)\nprint(\"\\nğŸ”¹ Test Metrics:\", test_metrics)\n\n# ğŸ”¹ Get Predictions\npredictions = trainer.predict(test_dataset)\npreds = np.argmax(predictions.predictions, axis=1)\nlabels = predictions.label_ids\n\n# ğŸ”¹ Print Evaluation Metrics\nprint(\"\\nğŸ”¹ Confusion Matrix:\")\nprint(confusion_matrix(labels, preds))\n\nprint(\"\\nğŸ”¹ Classification Report:\")\nprint(classification_report(labels, preds))\n\n# ğŸ”¹ Save Model & Tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T21:17:18.717073Z","iopub.execute_input":"2025-06-17T21:17:18.717674Z","iopub.status.idle":"2025-06-18T02:58:45.402460Z","shell.execute_reply.started":"2025-06-17T21:17:18.717636Z","shell.execute_reply":"2025-06-18T02:58:45.401861Z"}},"outputs":[{"name":"stderr","text":"2025-06-17 21:17:32.489364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750195052.664538      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750195052.716861      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ”¹ Training Severity Distribution:\nseverity\nØ­Ø±Ø¬        120301\nØºÙŠØ± Ø­Ø±Ø¬     38842\nName: count, dtype: int64\n\nğŸ”¹ Test Severity Distribution:\nseverity\nØ­Ø±Ø¬        30075\nØºÙŠØ± Ø­Ø±Ø¬     9711\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27df2eb07af844e99f18fd07172f02ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdecc84d63e94060925e1da02648e47d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/717k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4119d0607ca9472a9c6ba4a5cf3bb227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d31fc44efd014322ba2fa5375282f990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d52f6a2a7049acaa387c18f4f4252f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/159143 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fdd6341027c42b99b8b9ca5f3a0847b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39786 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c436f1944941ed97180d7224a98bc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aa3593094af443c9494e9928b05a891"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/251172634.py:144: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ”¹ Starting Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250617_211901-6gsti7w4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rokaia-emad-modern-sciences-and-arts-university/huggingface/runs/6gsti7w4' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/rokaia-emad-modern-sciences-and-arts-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rokaia-emad-modern-sciences-and-arts-university/huggingface' target=\"_blank\">https://wandb.ai/rokaia-emad-modern-sciences-and-arts-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rokaia-emad-modern-sciences-and-arts-university/huggingface/runs/6gsti7w4' target=\"_blank\">https://wandb.ai/rokaia-emad-modern-sciences-and-arts-university/huggingface/runs/6gsti7w4</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16580' max='16580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16580/16580 5:34:39, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.366300</td>\n      <td>0.399708</td>\n      <td>0.811592</td>\n      <td>0.848478</td>\n      <td>0.811592</td>\n      <td>0.821169</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.305200</td>\n      <td>0.407458</td>\n      <td>0.813226</td>\n      <td>0.855082</td>\n      <td>0.813226</td>\n      <td>0.823340</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.208600</td>\n      <td>0.702634</td>\n      <td>0.751697</td>\n      <td>0.853418</td>\n      <td>0.751697</td>\n      <td>0.769072</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.140800</td>\n      <td>0.446717</td>\n      <td>0.838612</td>\n      <td>0.862798</td>\n      <td>0.838612</td>\n      <td>0.845339</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.074900</td>\n      <td>0.753390</td>\n      <td>0.808098</td>\n      <td>0.859763</td>\n      <td>0.808098</td>\n      <td>0.819446</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.061600</td>\n      <td>0.728312</td>\n      <td>0.823455</td>\n      <td>0.862409</td>\n      <td>0.823455</td>\n      <td>0.832774</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.025200</td>\n      <td>1.073315</td>\n      <td>0.811165</td>\n      <td>0.858920</td>\n      <td>0.811165</td>\n      <td>0.821994</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.019900</td>\n      <td>1.065887</td>\n      <td>0.827854</td>\n      <td>0.863709</td>\n      <td>0.827854</td>\n      <td>0.836617</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.014600</td>\n      <td>1.121916</td>\n      <td>0.831599</td>\n      <td>0.865139</td>\n      <td>0.831599</td>\n      <td>0.839920</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.011700</td>\n      <td>1.190708</td>\n      <td>0.834816</td>\n      <td>0.865688</td>\n      <td>0.834816</td>\n      <td>0.842662</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ”¹ Test Metrics: {'eval_loss': 0.4467172920703888, 'eval_accuracy': 0.838611571909717, 'eval_precision': 0.8627982624296657, 'eval_recall': 0.838611571909717, 'eval_f1': 0.8453385886669512, 'eval_runtime': 147.7081, 'eval_samples_per_second': 269.356, 'eval_steps_per_second': 2.81, 'epoch': 10.0}\n\nğŸ”¹ Confusion Matrix:\n[[25313  4762]\n [ 1659  8052]]\n\nğŸ”¹ Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.94      0.84      0.89     30075\n           1       0.63      0.83      0.71      9711\n\n    accuracy                           0.84     39786\n   macro avg       0.78      0.84      0.80     39786\nweighted avg       0.86      0.84      0.85     39786\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def save_complete_model(model, tokenizer, severity_mapping, save_path):\n    model.base_model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    classifier_state = {\n        'classifier_state': model.classifier.state_dict(),\n        'num_labels': model.classifier.out_features\n    }\n    torch.save(classifier_state, f\"{save_path}/classifier_state.pt\")\n    with open(f\"{save_path}/severity_mapping.pkl\", \"wb\") as f:\n        pickle.dump(severity_mapping, f)\n\n# Save the model\nsave_complete_model(trainer.model, tokenizer, severity_mapping, \"baubmindlab/bert-base-arabert-Severity\")\n\n# ğŸ”¹ Load the Model for Inference\ndef load_complete_model(model_path):\n    base_model = AutoModel.from_pretrained(model_path)\n    classifier_state = torch.load(f\"{model_path}/classifier_state.pt\", map_location=torch.device('cpu'))\n    model = CustomModel(base_model, classifier_state['num_labels'])\n    model.classifier.load_state_dict(classifier_state['classifier_state'])\n    model.eval()\n    return model\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"baubmindlab/bert-base-arabert-Severity\")\nmodel2 = load_complete_model(\"baubmindlab/bert-base-arabert-Severity\")\nprint(\"âœ… Model Loaded Successfully!\")\n\n# ğŸ”¹ Function for Severity Predictions\ndef predict_severity(text, model, tokenizer, severity_mapping):\n    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n    logits = outputs[\"logits\"]\n    predicted_label = torch.argmax(logits, dim=-1).item()\n    severity_mapping_reverse = {v: k for k, v in severity_mapping.items()}\n    return severity_mapping_reverse[predicted_label]\n\n# ğŸ”¹ Sample Severity Predictions\ntest_samples = [\"Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\", \"ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\", \"Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\", \"Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\"]\nfor text in test_samples:\n    predicted_severity = predict_severity(text, model2, tokenizer, severity_mapping)\n    print(f\"\\nğŸ”¹ Input: {text}\")\n    print(f\"Predicted Severity: {predicted_severity}\")\n\n# ğŸ”¹ Zip Model for Download\nshutil.make_archive(\"baubmindlab/bert-base-arabert-Severity\", 'zip', \"baubmindlab/bert-base-arabert-Severity\")\nprint(\"\\nâœ… Model Saved & Zipped for Download!\")\n\n# Provide a download link\nfrom IPython.display import FileLink\nFileLink(r'baubmindlab/bert-base-arabert-Severity')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T02:58:55.400031Z","iopub.execute_input":"2025-06-18T02:58:55.400509Z","iopub.status.idle":"2025-06-18T02:59:24.104368Z","shell.execute_reply.started":"2025-06-18T02:58:55.400483Z","shell.execute_reply":"2025-06-18T02:59:24.103449Z"}},"outputs":[{"name":"stdout","text":"âœ… Model Loaded Successfully!\n\nğŸ”¹ Input: Ø§Ù„Ø§Ø¹Ø±Ø§Ø¶ Ø´Ø¯ÙŠØ¯Ù‡\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‰ Ø±Ø§Ø­Ù‡ ÙÙ‚Ø·\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: Ø­Ø§Ù„Ù‡ Ù…ØªÙˆØ³Ø·Ù‡\nPredicted Severity: Ø­Ø±Ø¬\n\nğŸ”¹ Input: Ù†Ø²ÙŠÙ Ø­Ø§Ø¯\nPredicted Severity: Ø­Ø±Ø¬\n\nâœ… Model Saved & Zipped for Download!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4153679558.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Provide a download link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileLink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mFileLink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'baubmindlab/bert-base-arabert-Severity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, url_prefix, result_html_prefix, result_html_suffix)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             raise ValueError(\"Cannot display a directory using FileLink. \"\n\u001b[0m\u001b[1;32m    396\u001b[0m               \"Use FileLinks to display '%s'.\" % path)\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot display a directory using FileLink. Use FileLinks to display 'baubmindlab/bert-base-arabert-Severity'."],"ename":"ValueError","evalue":"Cannot display a directory using FileLink. Use FileLinks to display 'baubmindlab/bert-base-arabert-Severity'.","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"FileLink(r'baubmindlab/bert-base-arabert-Severity.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T03:02:00.841982Z","iopub.execute_input":"2025-06-18T03:02:00.842314Z","iopub.status.idle":"2025-06-18T03:02:00.849204Z","shell.execute_reply.started":"2025-06-18T03:02:00.842280Z","shell.execute_reply":"2025-06-18T03:02:00.848347Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/baubmindlab/bert-base-arabert-Severity.zip","text/html":"<a href='baubmindlab/bert-base-arabert-Severity.zip' target='_blank'>baubmindlab/bert-base-arabert-Severity.zip</a><br>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}